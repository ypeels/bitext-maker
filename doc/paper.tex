% Yes, this requires flipping flags to compile for different conferences...
% But you can press Ctrl+T while editing in TeXworks without having to change windows!

% conditional compilation
\def\buildemnlp{0}
\def\buildamta{1}
\def\buildhcomp{2}
\def\buildtarget{\buildemnlp} % required to be a single character for \if

% emnlp2016.sty, included by header-emnlp.tex, does NOT tolerate ANY surrounding \else statements...
% an if/else if/... or ifcase statement would have been preferred...
% but \documentclass is declared in the header file, which results in some hidden error-checking:
    % unsupported \buildtarget will fail to build (``\usepackage before \documentclass'')
    % duplicate target values (e.g., emnlp=amta=0) will fail to build (``two \documentclass commands'')
\if \buildtarget \buildemnlp    
    \input{header-emnlp.tex}
    \newcommand\mycitep[1]{\cite{#1}}     % conference-specific ``glue code''
    \newcommand\mynewcite[1]{\newcite{#1}}
    \newcommand\amtaonly[1]{}
    \newcommand\emnlponly[1]{#1}
    \newcommand\hcomponly[1]{}
\fi
\if \buildtarget \buildamta    
    \input{header-amta.tex}
    \newcommand\mycitep[1]{\citep{#1}} 
    \newcommand\mynewcite[1]{\cite{#1}}  
    \newcommand\amtaonly[1]{#1}
    \newcommand\emnlponly[1]{}
    \newcommand\hcomponly[1]{}
\fi
\if \buildtarget \buildhcomp    
    \input{header-aaai.tex}
    \newcommand\mycitep[1]{\cite{#1}} 
    \newcommand\mynewcite[1]{\citeauthor{#1}~\shortcite{#1}}  
    \newcommand\amtaonly[1]{}
    \newcommand\emnlponly[1]{}
    \newcommand\hcomponly[1]{#1}
\fi
    
% Add support for snippets of \zh{中文}
\usepackage{CJK}
\usepackage[utf8]{inputenc}
\newcommand\zh[1]{\begin{CJK}{UTF8}{gbsn}#1\end{CJK}}

% this would allow clickable urls for my convenience - but many conferences forbid them?
% \usepackage{hyperref}


\begin{document}
\maketitle




\begin{abstract}
The large-scale generation of high-quality synthetic parallel corpora for data-driven machine translation is proposed.

A preliminary system for doing this is presented, 



syntactic complexity in the spirit of rule-based machine translation, 

as well as high-quality sentence pairs via templates, in the spirit of example-based machine translation.

Early results do not show much improvement.
This is because we need to scale up?

Future work is proposed, in particular, 
In particular, we believe this is amenable to crowdsourcing by ``amateur linguists.''

\end{abstract}



\section{Motivation} 

As early as Jelinek's (in)famous quote during the 1980's that ``{\em Every time I fire a linguist, my performance goes up},'' both linguists and linguistics have been increasingly marginalized in natural language processing (NLP). 
More recently, for example, \mynewcite{collobert:2011} mention the possibility ``{\em [ideally]...to learn from letter sequences rather than words},'' illustrating an AI-motivated desire to move away from even the most fundamental of linguistic concepts.
A specific instance of this overall trend is the gradual shift of machine translation (MT) away from classical linguistic rule-based approaches \mycitep{hutchins:2005} to contemporary machine learning approaches, which underlie statistical phrase-based systems like Moses \mycitep{moses} and more recent neural network systems \mycitep{sutskever:2014,bengio:2014}.  
But in the end, even the most cutting-edge machine learning MT system relies crucially on multilingual parallel corpora, typically taken from external sources like multilingual parliamentary proceedings. 
This results in what we perceive to be a disproportionate amount of effort being dedicated to the machine learning component of machine translation, with input corpora being treated almost as an afterthought.\footnote{
    Of course, these parallel corpora represent hundreds of translator-hours of work. 
    However, these translations are generally optimized for human consumption, not machine learning, with no regard for such niceties as sentence alignment.
    This is illustrated briefly in Section \ref{subsec:case_study}.
    }

One of the major advantages of data-driven MT is that it removes the need for linguists to develop a new set of rules for an MT system to support a new language.
In a sense, though, the rule-based MT linguist's job has really been  ``outsourced'' to third-party translators, who implicitly encode linguistic information in parallel corpora.
We believe there is still a significant ``in-house'' role for linguists to play in corpus-based machine translation, and that is to direct the crafting of high-quality, ``designer'' synthetic parallel corpora, leveraging technologies like crowdsourcing to maximize throughput.
Such a division of labor would let linguists be linguists, with MT system development proceeding in parallel.  



% although it is ``concrete'', this section will probably be the first to get cut if I run OVER length (which had previously been unimaginable, but is starting to look more and more likely)
% this is some stupid little side note I just noticed a couple weeks ago, but it is expanding to a huge size, because it's like the only concrete ``positive'' result I have...
%\amtaonly{
\subsection{Case Study}
\label{subsec:case_study}

% TODO: don't need to go into this much detail, do you? This would be the first section to trim, if it runs OVER length
% To find an example of suboptimally translated parallel corpora in the literature, one need not look far.

As an example of suboptimal parallel corpora being used for machine translation, consider the official Chinese to English baseline translation of the {\small \tt tst2010} data set\footnote{https://wit3.fbk.eu/score.php?release=2014-01} from the TED talks corpus, described in greater detail in Section \ref{sec:experiments}.
As a very crude way of assessing translation quality, we used the official evaluation software MultEval \mycitep{multeval} to compute scores for each translated line individually.
Upon manual inspection, the worst scoring individual lines (with TER scores over 150\%) were almost all found to correspond to poor human translations on the Chinese side of the gold corpus.\footnote{
    High TER scores indicated misaligned sentences as well as clerical errors.  In particular, the lecture with {\small \tt talkid} 767 had dozens of erroneous lines, in some of the worst cases having the same Chinese sentence copy/pasted three times on the same line. 
    The baseline Moses system tries valiantly, but it ultimately scores poorly for trying to produce a good machine translation of a poor human translation.
    Alternatively, one can interpret these errors as an additional source of noise in the human encoding of English into Chinese.
}
Such translations might suffice for viewing translated videos monolingually, but they are far from ideal for training a machine translation system.

One might expect such errors to be relatively well-tolerated in large training corpora. 
Nevertheless, this is particularly unsettling because the {\small \tt tst2010} data set has been used as the official development/tuning set for the past several IWSLT evaluation campaigns. 
It is also unclear what effect such noisy test corpora have on MT system evaluation and comparison.

In any case, the fact that these errors have persisted this long ({\small \tt tst2010} has been distributed with only minor changes for every IWSLT Chinese-English evaluation campaign since 2012) is a testament to how insidious these errors can be, even for a leading international conference. 
At the very least, this suggests a need for more careful post-editing of bitexts, particularly for development and test corpora.
More ideal, though, would be the creation of parallel corpora for the express purpose of training, tuning, and evaluating data-driven MT systems.



% TODO: display an example bad-sentence pair? If I don't, then what was the point of getting zh fonts working?? sigh

% I guess don't be concrete - don't say it... because then, you have to be more concrete about it...
	% I can elaborate if reviewers ask... but for now, let's just be concise...

% TODO: table with change; also list best and worst 
% caption should mention that the difference between 
    % ALTHOUGH you would expect that it would improve all models, it's not clear whether...err? hard to be precise here.
%Removing TED talk number 767 (over 200 lines), as well as 15 hand-picked lines with bad TER scores, 
%First, note that the scores improve, which is consistent with the observation that 
% something to keep in mind in case reviewers ask
%This results in a significant TER score change that is almost as large as the score difference between the highest and lowest ranking system from the evaluation campaign.  
%BLEU scores \mycitep{bleu}, on the other hand, did not change much.
% no, omit this, because DUH, if you change the test set, the scores will change. (it's like changing your ruler.)
% TODO: table and reference
%\begin{table}[]
%\centering
%\label{table:tst2010}
%\begin{tabular}{|l|l|l|}
%\hline
% & BLEU & TER  \\ \hline
%Full {\small \tt tst2010} & 11.2 & 77.0  \\ \hline
%Purged {\small \tt tst2010} & 11.8 & 71.3  \\ \hline
%%\hline
%%{\small \tt tst2014} &  &   \\ \hline
%% &  &   \\ \hline
%\end{tabular}
%\caption{By comparison, the difference between the baseline TER score and (not shown) } % TODO: caption
%\end{table}
% TODO: table of scores after purging. This is pretty crucial? why?
	% i think i THOUGHT this was crucial because i had to back up my claims concretely...
	% but the claim from the data is tenuous at best - of course the scores change if the test set changes! so WHAT?
	% BESIDES, I tried training and tuning with purged corpora, and the difference was minimal. it's quite possible that people just don't CARE
%} % \amtaonly




\subsection{Potential Benefits}
\label{subsec:benefits}

Large, high-quality synthetic parallel corpora could be useful as:

\begin{itemize}
\item Training sets for corpus-based MT systems, particularly for low-resource language pairs
\item Benchmark tasks for MT evaluation, in the spirit of the Facebook bAbI tasks in question answering \mycitep{bAbI}
\item Translation memories for computer-aided translation
\item ``Language archives'': New NLP methods are developed every few years; linguistic theories change every decade or two; parallel corpora live forever (e.g., the Rosetta Stone).
\end{itemize}
% sigh, this set of bullets seems much clearer than all those stupid sentences that came before

We believe that natural language processing in general and machine translation in particular are particularly amenable to artifical data generation. 
Unlike many machine learning applications (image recognition, finance), NLP data sets are manmade to begin with; humans themselves {\em are} the primary natural producers and consumers of raw written language data.
Translation is further privileged as perhaps the only NLP task for which millions of people (foreign-language learners) already undergo years of formal training, which has implications for scalability via crowdsourcing (Section \ref{sec:future}).

Other practical considerations shine a favorable light on parallel corpus generation.
As the inverse problem of translation, multilingual generation is also an easier task in some senses. 
For example, lexical ambiguity can be largely alleviated,\footnote{
    Ideally the generator would have {\em a priori} knowledge about whether it is writing a sentence pair about, say,  a (financial) bank or a (river) bank.
} and segmentation of languages like Chinese \mycitep{segmenter:2005,segmenter:2008} becomes a non-issue.
Parallel corpora also provide a natural avenue through which to interface contemporary MT systems with the countless linguist-years of expertise that have been invested into rule-based MT systems, as discussed in Section \ref{sec:related}.
And unlike the various competing MT methods, which are by nature competitive, corpora are inherently collaborative.

This paper is organized as follows.
Section \ref{sec:related} discusses previous related work,
Section \ref{sec:generator} describes our first attempt at implementing a parallel corpus generator,
Section \ref{sec:experiments} gives preliminary results, 
and Section \ref{sec:future} proposes future work.



\section{Related Work}
\label{sec:related}

The idea of using synthetic parallel corpora in machine translation is hardly new.
Previous attempts at creating synthetic parallel corpora have used rule-based MT (RBMT) systems \mycitep{hu:2007,dugast:2008,murakami:2009,rubino:2014} but the input to the RBMT systems was again dictated by external corpora in the desire to produce immediate results.
A longer-term approach that may pay dividends (Section \ref{subsec:benefits}) would be to run RBMT systems on specially chosen sentences for which they are known to produce high-quality translations, or perhaps eventually directly encoding their rules into carefully crafted parallel corpora.

Our parallel corpus generation system (Section \ref{sec:generator}) is very similar to the recursive equivalence class approach to example-based MT \mycitep{ebmtbook} used by \mynewcite{brown:99}. 
Our generator has more of a rule-based flavor, using a somewhat more varied grammar that also admits the use of participial noun modifiers.
But more importantly, we believe this line of work was ahead of its time and consider our main contribution to be bringing attention to its potential value to contemporary MT.  

Equivalence classing is in fact an integral part of the standard pipeline of Moses \mycitep{moses}, the open-source statistical MT system. 
During training, the {\small \tt mkcls} component adds ``{\em an `example-based' touch to the statistical approach}'' by clustering words into automatically-generated equivalence classes to ameliorate data sparsity issues \mycitep{och:98}.
However, these clusters are trained only at the word level and not for phrases, and the clusters themselves are typically only used during alignment.
Furthermore, since they are learned in an unsupervised manner from the parallel corpus, they can still benefit from the addition of more high-quality training examples, synthetic or otherwise.

% POTENTIAL REVIEWER QUESTION (assuming they even understand wtf I was talking about here): well, but you can add word cluster ids as factors/features... (http://www.statmt.org/moses/?n=Advanced.Models#ntoc6 - simply annotate words )
% my weak responses
    % supervised data almost always trumps unsupervised data, especially in NLP. there is no real substitute for REAL, in-domain data
    % also, you would still have to require the models to to learn from these noisy clusterings
    % actually, word+mkcls is still a LEXICALIZED translation factor; that's probably why it doesn't really help during the translation phase
        % class-LM (target sequence model over cluster-ids) seems like it might be more useful....BUT, this is for MONOLINGUAL target-side data, 
            % which generally doesn't need the help
            % which is a strange thing to do to begin with, taking clusterings that were trained on expensive bilingual corpora and using them on plentiful monolingual corpora
% a possibly stronger response: automatic clusters are also of lower quality than hand-crafted template variations, and there will likely be clustering errors
% side note: if mkcls clusters were so awesome, they'd be used as standard factors... so you'd have to think that they're not


% these links are too long for EMNLP. TODO: add them as footnotes for AMTA?
% http://www.fb10.uni-bremen.de/anglistik/langpro/kpml/genbank/generation-bank.html
% http://www.fb10.uni-bremen.de/anglistik/langpro/kpml/ genbank/R3b12-English/Docu/ENGLISH-nigel-exerciseset-mismatches-19981209/EX-SET-1.html
In terms of freely available multilingual natural language generators, the most well-developed one we could find was KPML \mycitep{kpml}.
But sentence writing is fairly involved because the focus seems to be on having a well-developed linguistic theory to explain the surface forms.
In contrast, our primary goal is to generate parallel corpora of surface forms, affording us a significant amount of flexibility as to the method of generation. 

In the next section, we describe our own primitive attempt at a multilingual generator. 
While we try to model straightforward linguistic phenomena compositionally, in the tradition of example-based MT \mycitep{ebmtbook}, we adopt the pragmatic attitude that sometimes the most effective way to represent a translation phenomenon is simply to write it down.
Thus, with the long-term goal of scalability, we attempt to construct a system sufficiently flexible to write sentence pairs in large quantities through syntactic variations and also with ``hand-crafted'' quality through example-based templates.


\section{A Nascent Multilingual Generator}
\label{sec:generator}



As an example of how our synthetic parallel corpus generation system works, consider the following English-Chinese template pair:

%\small
\begin{center} \begin{small} 
{\tt S V O .} \\
{\tt S \zh{把} O V \zh{。}}
\end{small} \end{center} 
%\normalsize

\noindent Our system then expands the subject {\small \tt S} and object {\small \tt O} as noun phrases, adding adjectival and participial modifiers as desired. 
Semantic constraints can be imposed from the verb {\small \tt V} as well as externally.
The generator can then vary the template lexically\footnote{Rewrite rule probabilities were set by hand; a more principled approach would, say, use rewrite probabilities from a generative PCFG parser.} and syntactically, allowing a large number of sentence pairs to be generated from a single template.
We can also trade quantity for quality by adding more literals to the template, in the spirit of example-based MT.

% TODO: looks like this is GOING to run over length. save some if not all of this for AMTA, which is going to run way UNDER length
%\subsection{Simplistic Linguistics}
%\amtaonly{
We adopt an exceedingly simplistic view of language as nouns, verbs, and their modifications and elaborations.
Inspired by early work on transformations by \mynewcite{chomsky}, we focus on declarative indicative present active sentences in the hopes that they can be transformed into other forms in the long run. 
Similarly, although we enforce some semantic constraints, our system generally produces nonsensical sentences like ``{\em Colorless green ideas sleep furiously.}''
The fact that verb phrases can also serve as nouns (gerundives) and modifiers (participles) appears to be a common source of complexity, and we made it a priority to implement participial modifiers.
To capture recursive structures like participles, we adopt a rudimentary tree representation that is something intermediate between constituency trees and dependency trees, with constituent-like modifiers that also maintain dependency-like links to their targets.
%}

%\subsection{Implementation Details}
% TODO: mention Python?
One implementation detail worth mentioning is the choice of YAML for writing sentence templates and lexical data. 
In particular, YAML has native Unicode support and does not even require quotation marks to enclose strings.

% TODO: should I bother with this? 
Crude though it may be, we plan on eventually open-sourcing both our generator and its accompanying data.

% oh this part really is just flying now, because there's no uncertainty as to what I DID

% running over length, but let's just write it all down and then see what to cut (hint: that stupid tst2010 stuff. maybe just summarize it in a footnote)


\section{Preliminary Experiments}
\label{sec:experiments}

%\subsection{Methods} % no space? just do one paragraph each?
We consider training set amplification as a first application.
Since we are interested in the relative effect of our synthetic parallel corpus, we chose a simple baseline system and fix all other variables.
Namely, we used the baseline configuration of Moses \mycitep{moses} as described in \mycitep{iwslt2015}, with the exception of changing the target language model to KenLM \mycitep{kenlm}, which we found to be faster and stabler than IRSTLM \mycitep{irstlm} at decoding time.
For the remaining unspecified model settings, we used the {\small \tt grow-diag} alignment heuristic recommended by \mynewcite{segmenter:2008} and the {\small \tt mslr-fe} configuration for lexicalized reordering. 
Chinese text was preprocessed using the latest version of the Stanford Segmenter \mycitep{segmenter:2005,segmenter:2008}.
Phrase and lexicalized reordering tables were interpolated using the {\small \tt tmcombine} script included with Moses \mycitep{tmcombine}, with the interpolation tuned on the {\small \tt dev2010} data set.
BLEU scores were computed as described by \mynewcite{iwslt2015} using the IWSLT 2015 progress data set ({\small \tt tst2014}).\footnote{We chose the {\small \tt tst2014} test set to facilitate comparison with systems from both IWSLT 2014 and 2015 in future work.}

%\subsection{Task}



% TODO: filler for AMTA? used for corpus analysis stanford POS tagger (incomplete bibtex entry?) \mycitep{postagger:2003}
We evaluate the contribution of our synthetic parallel corpora using the Chinese to English task of IWSLT 2015 \mycitep{iwslt2015}, mentioned briefly in Section \ref{subsec:case_study}.
This task uses the WIT$^3$ corpus \mycitep{wit3}, which consists of multilingual subtitles of TED talks covering a wide variety of subjects. 
As a control, we also add the freely available MultiUN corpus \mycitep{multiun} from the OPUS project \mycitep{opus:2012}.

% TODO: add OpenSubtitles 2016 results? they don't really add much to the discussion - I can just mention them briefly, maybe even in a footnote
% oh okay, this is nice - if I don't include OpenSubtitles, then I don't have to talk about the stupid purging either
%heuristically purged zh\_zh, which is a very noisy corpus
%zh\_zh corpus from the OpenSubtitles 2016 corpus , with lines cleaned using some ad hoc heuristics



%\subsection{Results}


% yeah sure, let's add Bing/Google results just for kicks, then take them out later

% definitely want 2 tables?
    % one for learning curves, to show that iwslt > zh_zh > multiun - domain overlap is greater
    
    % one for 100%, to show that the out of domain corpora aren't helping that much. need moar in-domain data (probably)
    % well, maybe i'll do away with this one
    
    % I guess don't bother showing single-corpus results for zh_zh and multiun, because then I'd have to show MY single-corpus results, which are even more embarrassing than the horribleness I've written up here


% when i actually go and READ the table, it's clear that I don't even want to LOOK at more than one metric. it's information overload.
%\begin{table}
%\centering
%\label{table:tst2010}
%\begin{tabular}{|l|r|r|r|}
%\hline
%                    & BLEU  & NIST & TER    \\ \hline
%IWSLT15 official    & 11.43 & 4.67 & 72.65  \\ \hline
%Our baseline        & 11.54 & 4.43 & 72.43  \\ \hline
%  + OpenSubtitles   & 11.54 & 4.43 & 72.43  \\ \hline
%  + MultiUN         & 11.67 & 4.48 & 72.60  \\ \hline
%  + generated       & 11.46 & 4.39 & 72.51  \\ \hline
%% &  &   \\ \hline
%\end{tabular}
%\caption{}
% 
%\end{table}


% side-by-side tables as in emnlp2016.tex
\begin{table}
\label{tab:results}
\centering
\small
\begin{tabular}{cc}

\begin{tabular}{|l|r|}
\hline
                    & BLEU   \\ \hline
25\% corpus         &  9.95  \\ \hline
  + 75\%            & \textbf{11.47}  \\ \hline
%  + Subtitles       & 10.57  \\ \hline
  + MultiUN         & 10.76  \\ \hline
  + Generated       &  9.91  \\ \hline
\end{tabular}

\begin{tabular}{|l|r|}
\hline                    & BLEU   \\ \hline
Official            & 11.43  \\ \hline
Our baseline        & 11.54  \\ \hline
%  + Subtitles       & 11.54  \\ \hline
  + MultiUN         & \textbf{11.67}  \\ \hline
  + Generated       & 11.46  \\ \hline
\end{tabular} & % oh this is how you get 2 tables side by side

\end{tabular} % cc (i.e., the big super-table)

\caption{
    (Left) Chinese-to-English BLEU scores for our baseline Moses configuration with phrase and reordering tables trained on a random 25\% subset of the IWSLT15 training corpus and interpolated (+) with tables for the remaining 75\% (\textasciitilde 150K) of the in-domain corpus, the MultiUN corpus (a \textasciitilde 1M line subset), and a synthetic corpus from our generator (\textasciitilde 5M lines).
    (Right) Same as on the left, but starting with tables for the entire IWSLT15 corpus.
    Official published scores of the IWSLT15 baseline system are also given for comparison.    
    Qualitatively similar trends for NIST and TER scores were also observed (omitted for clarity).
%    The ``baseline'' row gives 
%    MultiUN is actually more data because it has more words per line.
% automatically cleaned version of the en-zh_zh language pair of OpenSubtitles 2016, roughly 1M lines.
% and a similar number of lines from MultiUN, which is actually more words. but it doesn't do as well.
} % caption

\end{table}

% preemptively answering reviewer question: yeah yeah, the trained tmcombine weights are like 0.001 for my generated corpora
The results of these experiments are shown in Table \ref{tab:results}.
For the table on the left, note that adding in-domain data improves performance significantly more than a much larger amount of out-of-domain data (MultiUN), a commonly reported phenomenon \mycitep{tmcombine}.
As prepared lectures, TED talks fall somewhere between the conversational language of movie subtitles and the formal written language of UN resolutions; data from the OpenSubtitles 2016 corpus \mycitep{opensubtitles:2016}\footnote{http://www.opensubtitles.org/} from the OPUS project are similarly ineffective at improving performance (scores not shown).
For the table on the right, note also that the same out-of-domain corpus now gives a much more modest performance increase.
These observations suggest that amplification of in-domain corpora should still be beneficial, particularly for low-resource language pairs or domains.

Our generated corpora do not make a significant impact on the final BLEU score,\footnote{
    Results for the most recently generated synthetic corpus are shown; scores slightly higher than our baseline have been obtained on older generated corpora.
    Both of these outcomes seem to be within the error bars of the MERT tuning procedure.
}
indicating that our parallel corpus generator has a long way to go.
Development on our generator has so far focused on ``rule-based'' syntactic diversity and growing a lexical database tailored to the training set, 
Given these results, a change of direction seems in order; work on harvesting more templates from the IWSLT15 training corpus is currently in progress.



% TODO: maybe add Bing and Google results, since nobody seems to report these? save this for AMTA? (haha not gonna have room, sucka)






\section{Proposed Future Work}
\label{sec:future}

I.e., shit I REALLY doubt I have time for...


perhaps using example-based MT techniques \mycitep{ebmtbook} and crowdsourcing to generate them at scale.

For this paper, the lexical database was drawn from the IWSLT training set to maximize bottom-line performance.
[i.e., i don't have TIME to do a general-purpose database]
This is indeed one potential use of this system, for training set amplification.
The envisioned ultimate target domain is (cleaned) everyday speech (maybe movie or TV subtitles?) , for which syntactic and lexical diversity is expected to be relatively low, and unknown words would be particularly suitable for crowdsourcing.

parallel corpus construction has been considered prohibitively expensive, hence the use of pre-translated sources like parliamentary proceedings
we believe it is worth an initial investment to create a scalable system

expert-sourcing by linguists, using the computer to lever up - computer assisted translation, in a sense; maybe computer-amplified
crowdsourcing by ``amateur linguists''


moreover, you're at the mercy of your third-party translators - if you want more data, you gotta wait.
But with a sufficiently compelling educational app/platform, you wouldn't have to wait OR pay for more data

Crowdsourcing also has its place in post-editing, at the very least, for test sets.
In a shared evaluation task with many language pairs, the most practical way for this is probably crowdsourcing.

But must this be so? Vast untapped resource of students or whatever.
an open challenge to the NLP community at large
with a sufficiently compelling and thus widely used educational app 
a potentially game-changing source of supervised data for NLP in general

Well, typical approaches are paid, like Mechanical Turk... but there is a vast untapped source of data in the form of student problem sets
as with the success story of reCAPTCHA \mycitep{recaptcha}, we believe this will scale best when there is a natural harmony between data supply and demand

Sufficient quantities of supervised data seem to underlie high or state of the art performance in NLP

estimate about 1 noun per minute (my pathetic work speed)

locally weighted linear regression? online learning?
but corpus generation at test time is probably just as noisy as any discriminative method

Instead of the next paragraph, should probably just point out how rudimentary the current ``grammar'' is. 
Future development may be facilitated by adapting existing work from RBMT or NLG. 
Okay, I guess just point out this grammar doesn't even handle something as basic as:

Shit tons of linguistic phenomena that aren't even that uncommon, including but not limited to:
More thorough treatment of articles.
(More) treatment of prepositional phrases.
Conjunctions.
Interrogatives and imperatives.


In the short term
Increasing lexical variation by expanding the lexicon, especially for uncommon words in the training corpus, to alleviate data sparsity.
Increasing syntactic variation in a high-quality way by harvesting more templates, in the old EBMT tradition
Work along these lines is currently underway.


RNN encoder-decoders are not the first corpus-based MT methodology (EBMT then SMT then this), and it is unlikely in this era of Big Data that they will be the last.




% TODO: Wit^3 is not fully capitalized in stupid bibliography
% TODO: wtf, ``chinese'' is not capitalized in stupid bibliography


\amtaonly{
\section{AMTA}
This text only gets added for AMTA. I think it'd be nice to have this for ``filler'' text

If I really am submitting to this conference too, then I can afford (and probably want to?) draw out the length, 

Should I mention that you can kind of discern differences between IWSLT, movie subtitles, and MultiUN from their POS distributions? I would probably have to mention something concrete then...? 
} % end \amtaonly



\hcomponly{ % hmm, maybe I should use this stuff to pad AMTA too? well, let's not pad just for padding's sake... spare the reviewers
\section{HCOMP}

wait, the HCOMP ``short paper'' needs to be 2 pages at MOST, including references

An open challenge to the community - making a crowdsourcing platform
Should I do a quick back-of-the-envelope calculation on how many child-hours are spent on language-related problem sets?

We believe that for crowdsourcing to be most fruitful, there should a matching incentive to the crowd... 
At the risk of sounding like a sales pitch, what if the crowd wanted to pay you to give you data, like Tom Sawyer?
The 2015 paper only tagged like 200 words... well, they were trying something WordNet style... but it just goes to show how slow it is...
WordNet has shit tons of senses, but only a handful of them are common... we want the most common senses here, not completeness.
For rare senses, we can make custom templates.
The semantics we desire are more lightweight (?) - just enough to generate surface forms; don't need to do complex logical reasoning on it.

Rare nouns are the biggest source of lexical diversity.
VERY expensive to do with a small team.
Very ripe for crowdsourcing, but not necessarily by MOOCs, as proposed by LearnWork - problem sets seem more appropriate here.
With a sufficiently compelling problem set app, schools could possibly be willing to pay subscription fees to use this app... (hmm, I see a problem - schools don't have that much money... well, that's where scale comes into play?) well, at the very least, if you make it free, you still get their data
The question seems not so much ``should we'' as much as ``why aren't we doing this yet?'' - at least in the context of NLP.
Unlike some traditional machine learning tasks, which use either sensory or observed data as input, human writers ARE the primary and most natural source of linguistic data.
You wouldn't want to crowdsource bitmaps of cats.
And even if you were crowdsourcing photos of cats, or even photo labeling, that's not really something students do beyond preschool.
But to master one's native tongue takes like 10 years, and there's a BUNCH of unharnessed labor there.

} % end \hcomponly





% these are so short that it's probably better to have it in literally instead of using \input
\if \buildtarget \buildamta    
    \small
    \bibliographystyle{apalike}
\else \if \buildtarget \buildemnlp
    %Note that emnlp2016.bst chokes if there are 0 citations in your paper.    
    \mynewcite{moses}    
    \bibliographystyle{emnlp2016}
    
\else \if \buildtarget \buildhcomp    
    \bibliographystyle{aaai}
\else
    % some extra error checking, which could not be used with emnlp2016.sty
    \GenericError{}{Invalid buildtarget - \buildtarget}
\fi \fi \fi

\bibliography{references}

\end{document}
