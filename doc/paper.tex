% Yes, this requires flipping flags to compile for different conferences...
% But you can press Ctrl+T while editing in TeXworks without having to change windows!

% conditional compilation
\def\buildemnlp{0}
\def\buildamta{1}
\def\buildhcomp{2}
\def\buildtarget{\buildemnlp} % required to be a single character for \if

% emnlp2016.sty, included by header-emnlp.tex, does NOT tolerate ANY surrounding \else statements...
% an if/else if/... or ifcase statement would have been preferred...
% but \documentclass is declared in the header file, which results in some hidden error-checking:
    % unsupported \buildtarget will fail to build (``\usepackage before \documentclass'')
    % duplicate target values (e.g., emnlp=amta=0) will fail to build (``two \documentclass commands'')
\if \buildtarget \buildemnlp    
    \input{header-emnlp.tex}
    \newcommand\mycitep[1]{\cite{#1}}     % conference-specific ``glue code''
    \newcommand\mynewcite[1]{\newcite{#1}}
    \newcommand\amtaonly[1]{}
    \newcommand\hcomponly[1]{}
\fi
\if \buildtarget \buildamta    
    \input{header-amta.tex}
    \newcommand\mycitep[1]{\citep{#1}} 
    \newcommand\mynewcite[1]{\cite{#1}}  
    \newcommand\amtaonly[1]{#1}
    \newcommand\hcomponly[1]{}
\fi
\if \buildtarget \buildhcomp    
    \input{header-aaai.tex}
    \newcommand\mycitep[1]{\cite{#1}} 
    \newcommand\mynewcite[1]{\citeauthor{#1}~\shortcite{#1}}  
    \newcommand\amtaonly[1]{}
    \newcommand\hcomponly[1]{#1}
\fi
    
% Add support for snippets of \zh{中文}
\usepackage{CJK}
\usepackage[utf8]{inputenc}
\newcommand\zh[1]{\begin{CJK}{UTF8}{gbsn}#1\end{CJK}}

% this would allow clickable urls for my convenience - but many conferences forbid them?
% \usepackage{hyperref}


\begin{document}
\maketitle




\begin{abstract}
An increased focus on 

is proposed.

A particular system for doing this is presented.
It allows syntactic complexity via rules.
It also allows high-quality examples via templates, with minor syntactic modifications via rules.

Preliminary results do not show much improvement.
This is because we need to scale up?

Future work is proposed.
In particular, we believe this is amenable to crowdsourcing by ``amateur linguists''.

\end{abstract}



\section{Motivation} 

As early as Jelinek's (in)famous 1980's quote, ``{\em Every time I fire a linguist, my performance goes up},'' both linguists and linguistics have been increasingly marginalized in natural language processing. 
More recently, for example, \mynewcite{collobert 2011 footnote 8} mention the possibility of ``{\em [ideally]...to learn from letter sequences rather than words},'' illustrating an AI-motivated desire to move away from even the most fundamental of linguistic concepts.
A specific instance of this overall trend is the gradual shift of machine translation (MT) away from classical linguistic rule-based approaches \mycitep{hutchins} to contemporary machine learning approaches, which underlie statistical phrase-based systems like Moses \mycitep{moses} and more recent neural network systems \mycitep{sutskever, bengio}.  
But in the end, even the most cutting-edge machine learning MT system relies crucially on multilingual parallel corpora, typically taken from external sources like multilingual parliamentary proceedings. 
This results in what we perceive to be a disproportionate amount of effort being dedicated to the machine learning component of machine translation, with the ever-important input corpora being treated almost as an afterthought.\footnote{
    Of course, these parallel corpora represent hundreds of translator-hours of work. 
    However, these translations are generally optimized for human consumption, not machine learning, with no regard for such niceties as sentence alignment.
    This is illustrated briefly in Section \ref{case_study}.
    }


In this paper, we propose bringing linguists back from a premature exile to craft high-quality, ``designer'' synthetic parallel corpora for machine translation.
Such a division of labor would let linguists be linguists, with MT system development proceeding in parallel.  
In a sense, the use of external corpora has ``outsourced'' the rule-based MT linguist's job to third-party translators, who implicitly encode linguistic information in parallel corpora.  
What we propose is to bring these jobs back ``in-house'', in the updated context of corpus-based MT.

% TODO: i haven't talked yet about the unreasonable effectiveness of in-domain data...
% oh is that the ``upsides'' section?


% this is some stupid little side note I just noticed a couple weeks ago, but it is expanding to a huge size, because it's like the only concrete ``positive'' result I have...
\subsection{Case Study}
\label{case_study}

As a concrete example to illustrate the need for higher-quality parallel corpora, we briefly consider the official Chinese to English baseline translation of the {\small \tt tst2010} data set\footnote{https://wit3.fbk.eu/score.php?release=2014-01} from the IWSLT shared task studied in Section \ref{experiments}.
As a very crude way of assessing translation quality, we used the official evaluation software, MultEval \mycitep{multeval} to compute scores for each translated line individually.
Upon manual examination, it was found that the lines with the worst individual TER scores \mycitep{ter}, some as high as 300\%, almost all corresponded to significant translation errors in the Chinese side of the gold corpus.

Not only misaligned sentences, but also apparent data entry errors, 

Unreasonable for a human to duplicate, much less MT systems.

One would expect such outliers to be relatively well-tolerated in training sets
But the distressing part is that the {\small \tt tst2010} data set has been used as the official development/tuning set for the past several IWSLT evaluation campaigns.
It is unclear what effect such noisy corpora would have on other systems, 
especially when used for tuning
This results in a significant TER score change that is almost as large as the score difference between the highest and lowest ranking system from the evaluation campaign.  
BLEU scores, on the other hand, did not change much.

Removing TED talk number 767 (over 200 lines), as well as 15 hand-picked lines with bad TER scores, 


% TODO: table and reference






% TODO: display an example bad-sentence pair? 










Examining 


Just mention for now that there is an entire lecture that is bad
Also hand-purged several other sentences

TODO: table of scores after purging. This is pretty crucial





It is a testament to how indisidious these errors can be that they persisted this long (tst2010 was distributed as recently as IWSLT2015

Not even an international conference is immune to such issues

It also demonstrates the tremendous imbalance between linguistic effort and machine learning effort/investment




Higher corpus quality is desirable.
Larger in-domain corpus quantity is desirable too - see learning curve controls below?


\subsection{Upsides, Potential Applications}


My main point seems to be that nobody makes high quality and quantity parallel corpora JUST for machine translation, and it seems like that's sorely needed for both training and benchmarking.

Traditionally, corpus construction has been considered prohibitively expensive, ``scavenged'' from other sources like parliamentary proceedings or lecture subtitles.
But we argue here that it need not be so expensive.

Such synthetic parallel corpora can also be viewed as benchmark tasks (need to cite Facebook bAbI).
This is actually probably more important than you'd think, because looking at IWSLT15, some of the human translations are VERY loose.
So there's another element of noise in the data, and it's one that no amount of downstream machine learning can get rid of.
(Although people have tried corpus evaluation...)

% interesting, but probably confusing to readers?
%In the language of agile software development, test-driven MT system development, in which linguists pose sentence pairs, and MT developers work to translate them




This could take the form of amplified training sets in the short run, and perhaps a ``universal translation memory'' in the long run.

methods every few years; linguistic theories, representations every decade or so; parallel corpora live forever

lexical ambiguity is naturally alleviated

RBMT - countless developer-years of expertise

unlike all the different competing methods, it seems that a ``universal translation memory'' would be inherently collaborative (methods are competitive by nature, whereas data can be collaborative)


% moreover, you're at the mercy of your third-party translators - if you want more data, you gotta wait.










\section{Related Work}
\label{sec:related}

The basic 

The idea underlying generation system was essentially proposed 
This aspect of our work is perhaps closest in spirit to the equivalence classes used by \mynewcite{brown:99} to reduce the data requirements of an EBMT system \mycitep{ebmtbook}.

system described below is to have linguists write a template and then levering up with the computer to add lexical and syntactic variation.  
As a simple example, 
{\small \tt The NP jumped over the moon}





Unlike more AI-oriented or natural language understanding approaches, we are solely concerned here with the generation of surface forms





In the Moses toolchain, the {\small \tt mkcls} component \mycitep{och:98} performs this process in reverse, binning words into automatically-generated equivalence classes to alleviate data sparsity issues.
% USUALLY alignment only, CAN be applied to translation... http://www.statmt.org/moses/?n=Advanced.Models#ntoc6
% uh, so do people already do this??
With some effort, you can use these bins during steps other than alignment, although this is not done by default.
...
Moreover, as they are upstream of GIZA++, these bins are constructed at the lexical level, not the phrase level, whereas the templates described below can make substitutions at the level of larger constituents and phrases.
Furthermore, it is run at training time and cannot help with unknown words at test time (is this true??)
The only real cure for unknown words is moar data


some neural network MT paper?

aha, yoshua bengio's "curse of dimensionality" - becomes a blessing?  
Unlike many machine learning tasks (image recognition, finance), NLP data are purely artificial/manmade.  New paradigm?  Not enough attention has been paid IMO to semi-automated corpus generation.  Well, typical approaches are paid, like Mechanical Turk... but there is a vast untapped source of data in the form of student problem sets, in the spirit of reCAPTCHA (citation!?)
NLP is unique among machine learning tasks in that humans are not only consumers, but also producers of data - the alpha and the omega, the beginning and the end.
MT is further unique among NLP in that both input and output representations are widely accepted (modulo dialectical variations) - the source and target languages themselves.
People on the street would agree to the representations.


Translation is perhaps the only NLP task for which millions of people already undergo years of training

other approaches to synthetic corpora, which are essentially bootstrapped from other MT systems

Hardly a new idea ... but the emphasis here is on the resulting corpus





maybe the latest Hutchins overview or nutshell?


chomsky: transformations, colorless green ideas


size doesn't always matter - those massive OpenSubtitles corpora don't help
- uh, is the problem quality? should i run some Moses runs with a small portion of MultiUN to see how quality improves?

lots of things are resolved naturally when considering generation instead of discrimination
- ambiguity (you know what you want to say)
- segmentation (you know what ``phrases'' you wanted to output)

WELL... issues of ambiguity are alleviated, but only somewhat (e.g., for a sentence about a bank, the generator should know whether it's a building). 
BUT ambiguous attachments can still be generated (e.g., the [animal] with the [animal] with the overbite), and it would still take additional logic for the system to recognize this, much less generate all possible translations...
Maybe just emphasize the fact that lexical ambiguity is virtually eliminated, although structural or semantic ambiguity is still possible in some cases...
But it still seems like it'll be fewer cases... The [person] saw the dog with(by means of) the [vision implement]


Can think of this as something intermediate between fully automatic (RBMT, or semi-supervised SMT) and fully hand-crafted (human translation, which is the norm).
Compositional trees are more automatic-like, but generate more sentence pairs/tuples.
Complex templates are more hand-crafted, but generate fewer pairs/tuples, and are (presumably) harder to combine compositionally (but are they really!?).

The tree structure we adopt is something intermediate between constituency trees and dependency trees, and it was chosen for ease of implementation.

We adopt the pragmatic, ``defeatist'' attitude of modeling only straightforward linguistic phenomena compositionally; if the theory and data structures required to model a phenomenon are sufficiently complicated, everything is probably better off just going directly to a translation via template, instead of trying to concoct a sophisticated theory that has to be consistent with all previously theorized phenomena.

This is (probably? ref??) unlike other NLG work in that our emphasis is on surface forms, not the underlying data representation.


\section{Model}

used for corpus analysis stanford POS tagger (incomplete bibtex entry?) \mycitep{postagger:2003}

%\subsection{Simplistic Linguistics}
We adopt the exceedingly simplistic view of language as nouns, verbs, and descriptions (modifications) thereof.

two sources of complexity
- lexical (semantic?)
- syntactic 

The fact that verbs can serve as nouns (gerundives) and modifiers (participles and modals) is a significant source of added complexity. (footnote?)

Testing section reference: \ref{sec:related} - this requires changing secnumdepth=2 in AAAI

%\subsection{Implementation Details}
Python
YAML for data: very well suited. full Unicode support, and you don't even need to type quotation marks to enclose strings
Crude though it may be, we plan on eventually open-sourcing both the generator and its accompanying data





\section{Preliminary Experiments}
\label{experiments}

\subsection{Methods}
As mentioned in 
The IWSLT 2015 version of the WIT$^3$ corpus, which consists of multilingual subtitles of TED talks, covering a wide variety of subjects. 

As prepared lectures, they fall somewhere between the conversational language of movie subtitles and the formal written language of UN resolutions.
The latter are freely available online from the OPUS project \mycitep{opus} 

We consider the Chinese to English machine translation task, using the 

Out of domain corpus

the latest version of the Stanford Segmenter \mycitep{segmenter:2005,segmenter:2008}

citations needed
IWSLT \mycitep{wit3} - point out that this corpus is freely available
OPUS zh\_zh \mycitep{opensubtitles} MultiUN\mycitep{multiun} 
Moses \mycitep{moses}
KenLM \mycitep{kenlm}, which we found to be faster and stabler at decoding time.

parameters used

The corpus, which is freely available, consists 

IWSLT: TED talks are a series of lectures, which makes them 



heuristically purged zh\_zh, which is a very noisy corpus

\subsection{Results}

2016-02-20 learning curves adding only 130,000 in-domain lines improves scores by like 2.0 BLEU points

2016-03-03-225556 1M zh\_zh lines made scores slightly WORSE?

2016-04-29-093202 1M MultiUN lines only improved scores SLIGHTLY (by like 0.4 BLEU points)

Quantity and maybe even quality (you'd expect MultiUN translations to be better?) matters much less than having data in-domain

Of course, I need to redo (ALL?) these jobs with final segmenter choice

Evaluated using the IWSLT 2015 progress data set (tst2014) to allow comparison with both 2015 and 2014 systems.


\section{Proposed Future Work}

I.e., shit I REALLY doubt I have time for...


perhaps using example-based MT techniques \mycitep{ebmtbook} and crowdsourcing to generate them at scale.

For this paper, the lexical database was drawn from the IWSLT training set to maximize bottom-line performance.
[i.e., i don't have TIME to do a general-purpose database]
This is indeed one potential use of this system, for training set amplification.
The envisioned ultimate target domain is (cleaned) everyday speech (maybe movie or TV subtitles?) , for which syntactic and lexical diversity is expected to be relatively low, and unknown words would be particularly suitable for crowdsourcing.

expert-sourcing by linguists, using the computer to lever up - computer assisted translation, in a sense; maybe computer-amplified
crowdsourcing by ``amateur linguists''

Crowdsourcing also has its place in post-editing, at the very least, for test sets.
In a shared evaluation task with many language pairs, the most practical way for this is probably crowdsourcing.

But must this be so? Vast untapped resource of students or whatever.

estimate about 1 noun per minute (my pathetic work speed)

locally weighted linear regression? online learning?
but corpus generation at test time is probably just as noisy as any discriminative method

Instead of the next paragraph, should probably just point out how rudimentary the current ``grammar'' is. 
Future development may be facilitated by adapting existing work from RBMT or NLG. 
Okay, I guess just point out this grammar doesn't even handle something as basic as:

Shit tons of linguistic phenomena that aren't even that uncommon, including but not limited to:
More thorough treatment of articles.
(More) treatment of prepositional phrases.
Conjunctions.
Interrogatives and imperatives.
Pronouns - implemented as names here...? wait, but what about I/me? ugh

In the short term
Increasing lexical variation by expanding the lexicon, especially for uncommon words in the training corpus, to alleviate data sparsity.
Increasing syntactic variation in a high-quality way by harvesting more templates, in the old EBMT tradition
Work along these lines is currently underway.


%RNN encoder-decoders are not the first corpus-based MT methodology (EBMT then SMT then this), and it is unlikely in this era of Big Data that they will be the last.




% TODO: Wit^3 is not fully capitalized in stupid bibliography
% TODO: wtf, ``chinese'' is not capitalized in stupid bibliography


\amtaonly{
\section{AMTA}
This text only gets added for AMTA. I think it'd be nice to have this for ``filler'' text

If I really am submitting to this conference too, then I can afford (and probably want to?) draw out the length, 

Should I mention that you can kind of discern differences between IWSLT, movie subtitles, and MultiUN from their POS distributions? I would probably have to mention something concrete then...? 
} % end \amtaonly



\hcomponly{ % hmm, maybe I should use this stuff to pad AMTA too? well, let's not pad just for padding's sake... spare the reviewers
\section{HCOMP}

wait, the HCOMP ``short paper'' needs to be 2 pages at MOST, including references

An open challenge to the community - making a crowdsourcing platform
Should I do a quick back-of-the-envelope calculation on how many child-hours are spent on language-related problem sets?

We believe that for crowdsourcing to be most fruitful, there should a matching incentive to the crowd... 
At the risk of sounding like a sales pitch, what if the crowd wanted to pay you to give you data, like Tom Sawyer?
The 2015 paper only tagged like 200 words... well, they were trying something WordNet style... but it just goes to show how slow it is...
WordNet has shit tons of senses, but only a handful of them are common... we want the most common senses here, not completeness.
For rare senses, we can make custom templates.
The semantics we desire are more lightweight (?) - just enough to generate surface forms; don't need to do complex logical reasoning on it.

Rare nouns are the biggest source of lexical diversity.
VERY expensive to do with a small team.
Very ripe for crowdsourcing, but not necessarily by MOOCs, as proposed by LearnWork - problem sets seem more appropriate here.
With a sufficiently compelling problem set app, schools could possibly be willing to pay subscription fees to use this app... (hmm, I see a problem - schools don't have that much money... well, that's where scale comes into play?) well, at the very least, if you make it free, you still get their data
The question seems not so much ``should we'' as much as ``why aren't we doing this yet?'' - at least in the context of NLP.
Unlike some traditional machine learning tasks, which use either sensory or observed data as input, human writers ARE the primary and most natural source of linguistic data.
You wouldn't want to crowdsource bitmaps of cats.
And even if you were crowdsourcing photos of cats, or even photo labeling, that's not really something students do beyond preschool.
But to master one's native tongue takes like 10 years, and there's a BUNCH of unharnessed labor there.

} % end \hcomponly





% these are so short that it's probably better to have it in literally instead of using \input
\if \buildtarget \buildamta    
    \small
    \bibliographystyle{apalike}
\else \if \buildtarget \buildemnlp
    %Note that emnlp2016.bst chokes if there are 0 citations in your paper.    
    %\mynewcite{moses}    
    \bibliographystyle{emnlp2016}
    
\else \if \buildtarget \buildhcomp    
    \bibliographystyle{aaai}
\else
    % some extra error checking, which could not be used with emnlp2016.sty
    \GenericError{}{Invalid buildtarget - \buildtarget}
\fi \fi \fi

\bibliography{references}

\end{document}
