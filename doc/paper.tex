% Yes, this requires flipping flags to compile for different conferences...
% But you can press Ctrl+T while editing in TeXworks without having to change windows!

% conditional compilation
\def\buildemnlp{0}
\def\buildamta{1}
\def\buildhcomp{2}
\def\buildtarget{\buildemnlp} % required to be a single character for \if

% emnlp2016.sty, included by header-emnlp.tex, does NOT tolerate ANY surrounding \else statements...
% an if/else if/... or ifcase statement would have been preferred...
% but \documentclass is declared in the header file, which results in some hidden error-checking:
    % unsupported \buildtarget will fail to build (``\usepackage before \documentclass'')
    % duplicate target values (e.g., emnlp=amta=0) will fail to build (``two \documentclass commands'')
\if \buildtarget \buildemnlp    
    \input{header-emnlp.tex}
    \newcommand\mycitep[1]{\cite{#1}}     % conference-specific ``glue code''
    \newcommand\mynewcite[1]{\newcite{#1}}
    \newcommand\amtaonly[1]{} % currently being used to trim down to 4 pages
    \newcommand\emnlponly[1]{#1}
    \newcommand\hcomponly[1]{}
\fi
\if \buildtarget \buildamta    
    \input{header-amta.tex}
    \newcommand\mycitep[1]{\citep{#1}} 
    \newcommand\mynewcite[1]{\cite{#1}}  
    \newcommand\amtaonly[1]{#1}
    \newcommand\emnlponly[1]{}
    \newcommand\hcomponly[1]{}
\fi
\if \buildtarget \buildhcomp    
    \input{header-aaai.tex}
    \newcommand\mycitep[1]{\cite{#1}} 
    \newcommand\mynewcite[1]{\citeauthor{#1}~\shortcite{#1}}  
    \newcommand\amtaonly[1]{}
    \newcommand\emnlponly[1]{}
    \newcommand\hcomponly[1]{#1}
\fi
    
% Add support for snippets of \zh{中文}
\usepackage{CJK}
\usepackage[utf8]{inputenc}
\newcommand\zh[1]{\begin{CJK}{UTF8}{gbsn}#1\end{CJK}}

% this would allow clickable urls for my convenience - but many conferences forbid them?
% \usepackage{hyperref}


\begin{document}
\maketitle




% TODO: follow the 300-word website limit as a guideline to abstract length? check pdf guide
\begin{abstract}
The large-scale generation of synthetic parallel corpora for data-driven machine translation is proposed.
A rudimentary system for doing this is presented, with mechanisms for syntactic complexity in the spirit of classical rule-based machine translation and high-quality sentence pairs in the spirit of example-based machine translation.
Preliminary experiments integrating these synthetic corpora into a shared task are reported.
Promising future directions are discussed, in particular, large-scale contributions from ``amateur linguists'' via crowdsourcing.
\end{abstract}



\section{Motivation} 

As early as Jelinek's (in)famous quote during the 1980's that ``{\em Every time I fire a linguist, my performance goes up},'' both linguists and linguistics have been increasingly marginalized in natural language processing (NLP). 
More recently, for example, \mynewcite{collobert:2011} mention the possibility ``{\em [ideally]...to learn from letter sequences rather than words},'' illustrating an AI-motivated desire to move away from even the most fundamental of linguistic concepts.
A specific instance of this overall trend is the gradual shift of machine translation (MT) away from classical linguistic rule-based approaches \mycitep{hutchins:2005} to contemporary machine learning approaches, which underlie statistical phrase-based systems like Moses \mycitep{moses} and more recent neural network systems \mycitep{sutskever:2014,bengio:2014}.  
But in the end, even the most cutting-edge machine learning MT system relies crucially on multilingual parallel corpora, typically taken from external sources like multilingual parliamentary proceedings. 
This results in what we perceive to be a disproportionate amount of effort being dedicated to the machine learning component of machine translation, with input corpora being treated almost as an afterthought.\footnote{
    Of course, these parallel corpora represent hundreds of translator-hours of work. 
    However, these translations are generally optimized for human consumption, not machine learning, with no regard for such niceties as sentence alignment.
    \amtaonly{This is illustrated briefly in Section \ref{subsec:case_study}.}
    }
    
% it's TEMPTING to say this, but the same is essentially true of all supervised data sets - they're pretty much all human-labeled...
%It is surprising to say the least to contrast 
%technological sophistication
%
%with the direct human translation 
%whereas parallel corpus acquisition is still more or less entirely dependent on .



One of the major advantages of data-driven MT is that it removes the need for linguists to develop a new set of rules for an MT system to support a new language.
In a sense, though, the rule-based MT linguist's job has really been  ``outsourced'' to third-party translators, who implicitly encode linguistic information in parallel corpora.
We believe there is still a significant ``in-house'' role for linguists to play in corpus-based machine translation, and that is to direct the crafting of high-quality, ``designer'' synthetic parallel corpora, leveraging technologies like crowdsourcing to maximize throughput.
Such a division of labor would let linguists be linguists, with MT system development proceeding in parallel.  



% although it is ``concrete'', this section will probably be the first to get cut if I run OVER length (which had previously been unimaginable, but is starting to look more and more likely)
% this is some stupid little side note I just noticed a couple weeks ago, but it is expanding to a huge size, because it's like the only concrete ``positive'' result I have...
\amtaonly{
\subsection{Case Study}
\label{subsec:case_study}

% TODO: don't need to go into this much detail, do you? This would be the first section to trim, if it runs OVER length
% To find an example of suboptimally translated parallel corpora in the literature, one need not look far.

As an example of suboptimal parallel corpora being used for machine translation, consider the official Chinese to English baseline translation of the {\small \tt tst2010} data set\footnote{https://wit3.fbk.eu/score.php?release=2014-01} from the TED talks corpus, described in greater detail in Section \ref{sec:experiments}.
As a very crude way of assessing translation quality, we used the official evaluation software MultEval \mycitep{multeval} to compute scores for each translated line individually.
Upon manual inspection, the worst scoring individual lines (with TER scores over 150\%) were almost all found to correspond to poor human translations on the Chinese side of the gold corpus.\footnote{
    High TER scores indicated misaligned sentences as well as clerical errors.  In particular, the lecture with {\small \tt talkid} 767 had dozens of erroneous lines, in some of the worst cases having the same Chinese sentence copy/pasted three times on the same line. 
    The baseline Moses system tries valiantly, but it ultimately scores poorly for trying to produce a good machine translation of a poor human translation.
    Alternatively, one can interpret these errors as an additional source of noise in the human encoding of English into Chinese.
}
Such translations might suffice for viewing translated videos monolingually, but they are far from ideal for training a machine translation system.

One might expect such errors to be relatively well-tolerated in large training corpora. 
Nevertheless, this is particularly unsettling because the {\small \tt tst2010} data set has been used as the official development/tuning set for the past several IWSLT evaluation campaigns. 
It is also unclear what effect such noisy test corpora have on MT system evaluation and comparison.

In any case, the fact that these errors have persisted this long ({\small \tt tst2010} has been distributed with only minor changes for every IWSLT Chinese-English evaluation campaign since 2012) is a testament to how insidious these errors can be, even for a leading international conference. 
At the very least, this suggests a need for more careful post-editing of bitexts, particularly for development and test corpora.
More ideal, though, would be the creation of parallel corpora for the express purpose of training, tuning, and evaluating data-driven MT systems.



% TODO: display an example bad-sentence pair? If I don't, then what was the point of getting zh fonts working?? sigh

% I guess don't be concrete - don't say it... because then, you have to be more concrete about it...
	% I can elaborate if reviewers ask... but for now, let's just be concise...

% TODO: table with change; also list best and worst 
% caption should mention that the difference between 
    % ALTHOUGH you would expect that it would improve all models, it's not clear whether...err? hard to be precise here.
%Removing TED talk number 767 (over 200 lines), as well as 15 hand-picked lines with bad TER scores, 
%First, note that the scores improve, which is consistent with the observation that 
% something to keep in mind in case reviewers ask
%This results in a significant TER score change that is almost as large as the score difference between the highest and lowest ranking system from the evaluation campaign.  
%BLEU scores \mycitep{bleu}, on the other hand, did not change much.
% no, omit this, because DUH, if you change the test set, the scores will change. (it's like changing your ruler.)
% TODO: table and reference
%\begin{table}[]
%\centering
%\label{table:tst2010}
%\begin{tabular}{|l|l|l|}
%\hline
% & BLEU & TER  \\ \hline
%Full {\small \tt tst2010} & 11.2 & 77.0  \\ \hline
%Purged {\small \tt tst2010} & 11.8 & 71.3  \\ \hline
%%\hline
%%{\small \tt tst2014} &  &   \\ \hline
%% &  &   \\ \hline
%\end{tabular}
%\caption{By comparison, the difference between the baseline TER score and (not shown) } % TODO: caption
%\end{table}
% TODO: table of scores after purging. This is pretty crucial? why?
	% i think i THOUGHT this was crucial because i had to back up my claims concretely...
	% but the claim from the data is tenuous at best - of course the scores change if the test set changes! so WHAT?
	% BESIDES, I tried training and tuning with purged corpora, and the difference was minimal. it's quite possible that people just don't CARE
} % \amtaonly




\subsection{Potential Benefits}
\label{subsec:benefits}

Large, high-quality synthetic parallel corpora could be useful as:

\begin{itemize}
\item Training sets for corpus-based MT systems, particularly for low-resource language pairs
\item Benchmark tasks for MT evaluation, in the spirit of the Facebook bAbI tasks in question answering \mycitep{bAbI}
\item Translation memories for computer-aided translation
\item ``Language archives'': New NLP methods are developed every few years; linguistic theories change every decade or two; parallel corpora live forever (e.g., the Rosetta Stone).
\end{itemize}
% sigh, this set of bullets seems much clearer than all those stupid sentences that came before

We believe that natural language processing in general and machine translation in particular are particularly amenable to artifical data generation. 
Unlike many machine learning applications (image recognition, finance), NLP data sets are manmade to begin with; humans themselves {\em are} the primary natural producers and consumers of raw written language data.
Translation is further privileged as perhaps the only NLP task for which millions of people (foreign-language learners) already undergo years of formal training, which has implications for scalability via crowdsourcing (Section \ref{sec:future}).

Other practical considerations shine a favorable light on parallel corpus generation.
As the inverse problem of translation, multilingual generation is also an easier task in some senses. 
For example, lexical ambiguity can be largely alleviated,\footnote{
    Ideally the generator would have {\em a priori} knowledge about whether it is writing a sentence pair about, say,  a (financial) bank or a (river) bank.
} and segmentation of languages like Chinese \mycitep{segmenter:2005,segmenter:2008} becomes a non-issue.
Parallel corpora also provide a natural avenue through which to interface contemporary MT systems with the countless linguist-years of expertise that have been invested into rule-based MT systems, as discussed in Section \ref{sec:related}.
And unlike the various competing MT methods, which are by nature competitive, corpora are inherently collaborative.

This paper is organized as follows.
Section \ref{sec:related} discusses previous related work,
Section \ref{sec:generator} describes our first attempt at implementing a parallel corpus generator,
Section \ref{sec:experiments} gives preliminary results, 
and Section \ref{sec:future} proposes future work.



\section{Related Work}
\label{sec:related}

The idea of using synthetic parallel corpora in machine translation is hardly new.
Previous attempts at creating synthetic parallel corpora have used rule-based MT (RBMT) systems \mycitep{hu:2007,dugast:2008,murakami:2009,rubino:2014} but the input to the RBMT systems was again dictated by external corpora in the desire to produce immediate results.
A longer-term approach that may pay dividends (Section \ref{subsec:benefits}) would be to run RBMT systems on specially chosen sentences for which they are known to produce high-quality translations, or perhaps eventually directly encoding their rules into carefully crafted parallel corpora.

Our parallel corpus generation system (Section \ref{sec:generator}) is very similar to the recursive equivalence class approach to example-based MT \mycitep{ebmtbook} used by \mynewcite{brown:99}. 
Our generator has more of a rule-based flavor, using a somewhat more varied grammar that also admits the use of participial noun modifiers.
But more importantly, we believe this line of work was ahead of its time and consider our main contribution to be bringing attention to its potential value to contemporary MT.  

Equivalence classing is in fact an integral part of the standard pipeline of Moses \mycitep{moses}, the open-source statistical MT system. 
During training, the {\small \tt mkcls} component adds ``{\em an `example-based' touch to the statistical approach}'' by clustering words into automatically-generated equivalence classes to ameliorate data sparsity issues \mycitep{och:98}.
However, these clusters are trained only at the word level and not for phrases, and the clusters themselves are typically only used during alignment.
Furthermore, since they are learned in an unsupervised manner from the parallel corpus, they can still benefit from the addition of more high-quality training examples, synthetic or otherwise.

% POTENTIAL REVIEWER QUESTION (assuming they even understand wtf I was talking about here): well, but you can add word cluster ids as factors/features... (http://www.statmt.org/moses/?n=Advanced.Models#ntoc6 - simply annotate words )
% my weak responses
    % supervised data almost always trumps unsupervised data, especially in NLP. there is no real substitute for REAL, in-domain data
    % also, you would still have to require the models to to learn from these noisy clusterings
    % actually, word+mkcls is still a LEXICALIZED translation factor; that's probably why it doesn't really help during the translation phase
        % class-LM (target sequence model over cluster-ids) seems like it might be more useful....BUT, this is for MONOLINGUAL target-side data, 
            % which generally doesn't need the help
            % which is a strange thing to do to begin with, taking clusterings that were trained on expensive bilingual corpora and using them on plentiful monolingual corpora
% a possibly stronger response: automatic clusters are also of lower quality than hand-crafted template variations, and there will likely be clustering errors
% side note: if mkcls clusters were so awesome, they'd be used as standard factors... so you'd have to think that they're not


% these links are too long for EMNLP. TODO: add them as footnotes for AMTA?
% http://www.fb10.uni-bremen.de/anglistik/langpro/kpml/genbank/generation-bank.html
% http://www.fb10.uni-bremen.de/anglistik/langpro/kpml/ genbank/R3b12-English/Docu/ENGLISH-nigel-exerciseset-mismatches-19981209/EX-SET-1.html
In terms of freely available multilingual natural language generators, the most well-developed one we could find was KPML \mycitep{kpml}.
But sentence writing is fairly involved because the focus seems to be on having a well-developed linguistic theory to explain the surface forms.
In contrast, our primary goal is to generate parallel corpora of surface forms, affording us a significant amount of flexibility as to the method of generation. 

In the next section, we describe our own primitive attempt at a multilingual generator. 
While we try to model straightforward linguistic phenomena compositionally, in the tradition of example-based MT \mycitep{ebmtbook}, we adopt the pragmatic attitude that sometimes the most effective way to represent a translation phenomenon is simply to write it down.
Thus, with the long-term goal of scalability, we attempt to construct a system sufficiently flexible to write sentence pairs in large quantities through syntactic variations and also with ``hand-crafted'' quality through example-based templates.


\section{A Nascent Multilingual Generator}
\label{sec:generator}



As an example of how our synthetic parallel corpus generation system works, consider the following English-Chinese template pair:

%\small
\begin{center} \begin{small} 
{\tt S V O .} \\
{\tt S \zh{把} O V \zh{。}}
\end{small} \end{center} 
%\normalsize

\noindent Our system then expands the subject {\small \tt S} and object {\small \tt O} as noun phrases, adding adjectival and participial modifiers as desired. 
Semantic constraints can be imposed from the verb {\small \tt V} as well as externally.
The generator can then vary the template lexically\footnote{Rewrite rule probabilities were set by hand; a more principled approach would, say, use rewrite probabilities from a generative PCFG parser.} and syntactically, allowing a large number of sentence pairs to be generated from a single template.
We can also trade quantity for quality by adding more literals to the template, in the spirit of example-based MT.

% TODO: looks like this is GOING to run over length. save some if not all of this for AMTA, which is going to run way UNDER length
%\subsection{Simplistic Linguistics}
\amtaonly{
We adopt an exceedingly simplistic view of language as nouns, verbs, and their modifications and elaborations.
Inspired by early work on transformations by \mynewcite{chomsky}, we focus on declarative indicative present active sentences in the hopes that they can be transformed into other forms in the long run. 
Similarly, although we enforce some semantic constraints, our system generally produces nonsensical sentences like Chomsky's celebrated ``{\em Colorless green ideas sleep furiously.}''
The fact that verb phrases can also serve as nouns (gerundives) and modifiers (participles) appears to be a common source of complexity, and we made it a priority to implement participial modifiers.
To capture recursive structures like participles, we adopt a rudimentary tree representation that is something intermediate between constituency trees and dependency trees, with constituent-like modifiers that also maintain dependency-like links to their targets.
}

%\subsection{Implementation Details}
% TODO: mention Python?
We plan on eventually open-sourcing both our generator and its accompanying data.
One implementation detail worth mentioning is the choice of YAML for writing sentence templates and lexical data. 
In particular, YAML has native Unicode support and does not even require quotation marks to enclose strings.

% TODO: should I bother with this? 


% oh this part really is just flying now, because there's no uncertainty as to what I DID

% running over length, but let's just write it all down and then see what to cut (hint: that stupid tst2010 stuff. maybe just summarize it in a footnote)


\section{Preliminary Experiments}
\label{sec:experiments}

%\subsection{Methods} % no space? just do one paragraph each?
We now consider training set amplification as a first application, using the Chinese to English MT task of IWSLT 2015 \mycitep{iwslt2015}\emnlponly{.}\amtaonly{, mentioned briefly in Section \ref{subsec:case_study}.}
This task uses the WIT$^3$ corpus \mycitep{wit3}, which consists of multilingual subtitles of TED talks covering a wide variety of subjects. 
The freely available MultiUN corpus \mycitep{multiun} from the OPUS project \mycitep{opus:2012} was also used for comparison.

To investigate the relative effect of our synthetic parallel corpora, we chose a simple baseline system and fixed all other variables.
Namely, we used the baseline configuration of Moses \mycitep{moses} as described in \mycitep{iwslt2015}, with the exception of changing the target language model to KenLM \mycitep{kenlm}, which we found to be faster and stabler at decoding time than IRSTLM \mycitep{irstlm}.
For the remaining unspecified model settings, we used the {\small \tt grow-diag} alignment heuristic recommended by \mynewcite{segmenter:2008} and the {\small \tt mslr-fe} configuration for lexicalized reordering. 
Chinese text was preprocessed using the latest version of the Stanford Segmenter \mycitep{segmenter:2005,segmenter:2008}.
Phrase and lexicalized reordering tables were interpolated using the {\small \tt tmcombine} script included with Moses \mycitep{tmcombine}, with the interpolation tuned on the {\small \tt dev2010} data set.
BLEU scores were computed as described by \mynewcite{iwslt2015} using the IWSLT 2015 progress data set ({\small \tt tst2014}).\footnote{We chose the {\small \tt tst2014} test set to facilitate comparison with systems from both IWSLT 2014 and 2015 in future work.}

%\subsection{Task}



% TODO: filler for AMTA? used for corpus analysis stanford POS tagger (incomplete bibtex entry?) \mycitep{postagger:2003}


% TODO: add OpenSubtitles 2016 results? they don't really add much to the discussion - I can just mention them briefly, maybe even in a footnote
% oh okay, this is nice - if I don't include OpenSubtitles, then I don't have to talk about the stupid purging either
%heuristically purged zh\_zh, which is a very noisy corpus
%zh\_zh corpus from the OpenSubtitles 2016 corpus , with lines cleaned using some ad hoc heuristics



%\subsection{Results}


% yeah sure, let's add Bing/Google results just for kicks, then take them out later

% definitely want 2 tables?
    % one for learning curves, to show that iwslt > zh_zh > multiun - domain overlap is greater
    
    % one for 100%, to show that the out of domain corpora aren't helping that much. need moar in-domain data (probably)
    % well, maybe i'll do away with this one
    
    % I guess don't bother showing single-corpus results for zh_zh and multiun, because then I'd have to show MY single-corpus results, which are even more embarrassing than the horribleness I've written up here


% when i actually go and READ the table, it's clear that I don't even want to LOOK at more than one metric. it's information overload.
%\begin{table}
%\centering
%\label{table:tst2010}
%\begin{tabular}{|l|r|r|r|}
%\hline
%                    & BLEU  & NIST & TER    \\ \hline
%IWSLT15 official    & 11.43 & 4.67 & 72.65  \\ \hline
%Our baseline        & 11.54 & 4.43 & 72.43  \\ \hline
%  + OpenSubtitles   & 11.54 & 4.43 & 72.43  \\ \hline
%  + MultiUN         & 11.67 & 4.48 & 72.60  \\ \hline
%  + generated       & 11.46 & 4.39 & 72.51  \\ \hline
%% &  &   \\ \hline
%\end{tabular}
%\caption{}
% 
%\end{table}


% side-by-side tables as in emnlp2016.tex
\begin{table}
\centering
\small
\begin{tabular}{cc}

\begin{tabular}{|l|r|}
\hline
                    & BLEU   \\ \hline
25\% corpus         &  9.95  \\ \hline
  + 75\%            & \textbf{11.47}  \\ \hline
%  + Subtitles       & 10.57  \\ \hline
  + MultiUN         & 10.76  \\ \hline
  + Generated       &  9.91  \\ \hline
\end{tabular}

\begin{tabular}{|l|r|}
\hline                    & BLEU   \\ \hline
Official            & 11.43  \\ \hline
Our baseline        & 11.54  \\ \hline
%  + Subtitles       & 11.54  \\ \hline
  + MultiUN         & \textbf{11.67}  \\ \hline
  + Generated       & 11.46  \\ \hline
\end{tabular} & % oh this is how you get 2 tables side by side

\end{tabular} % cc (i.e., the big super-table)


% TODO: mention that official results are on a slightly different Moses configuration? mehhhhh
\caption{ \label{tab:results} % oh, for reference numbering, label goes INSIDE caption
    (Left) Chinese-to-English BLEU scores for our baseline Moses configuration with phrase and reordering tables trained on a random 25\% subset of the IWSLT15 training corpus and interpolated (+) with tables for the remaining 75\% (\textasciitilde 150K) of the in-domain corpus, the MultiUN corpus (a \textasciitilde 1M line subset), and a synthetic corpus from our generator (\textasciitilde 5M lines).
    (Right) Same as the left, but starting with tables trained on the entire IWSLT15 corpus.
    Official published scores of the IWSLT15 baseline system are also given for comparison.    
    Qualitatively similar trends for NIST and TER scores were also observed (omitted for clarity).
%    The ``baseline'' row gives 
%    MultiUN is actually more data because it has more words per line.
% automatically cleaned version of the en-zh_zh language pair of OpenSubtitles 2016, roughly 1M lines.
% and a similar number of lines from MultiUN, which is actually more words. but it doesn't do as well.
} % caption

\end{table}

% preemptively answering reviewer question: yeah yeah, the trained tmcombine weights are like 0.001 for my generated corpora
The results of these experiments are shown in Table~\ref{tab:results}.
For the table on the left, note that adding in-domain data improves performance significantly more than a much larger amount of out-of-domain data (MultiUN), a commonly reported phenomenon \mycitep{tmcombine}.
As prepared lectures, TED talks fall somewhere between the conversational language of movie subtitles and the formal written language of UN resolutions; data from the OpenSubtitles\footnote{http://www.opensubtitles.org/} 2016 corpus \mycitep{opensubtitles:2016} from the OPUS project were similarly less effective at improving performance than a much smaller amount of in-domain data (scores were similar to MultiUN and are not shown).
For the table on the right, note the diminishing returns of out-of-domain data as the amount of in-domain data increases.
These observations underscore the importance of domain overlap and hint at the  potential value to be found in the amplification of in-domain corpora.

Our generated corpora, however, do not make a significant impact on the final BLEU score,\footnote{
    The sub-baseline scores shown are for our most recently generated synthetic corpus; scores slightly higher than our baseline have also been obtained on older generated corpora.
    Both of these outcomes seem to be within experimental error bars.
}
indicating that our parallel corpus generator has a long way to go.
Development on our generator has so far focused on ``rule-based'' syntactic diversity and growing a lexical database tailored to the training set. 
Given the resulting scores, a change of direction seems in order; work on harvesting more templates from the IWSLT15 training corpus is currently in progress.



% TODO: maybe add Bing and Google results, since nobody seems to report these? save this for AMTA? (haha not gonna have room, sucka)






\section{Proposed Future Work}
\label{sec:future}

\amtaonly{UNDER CONSTRUCTION (basically threw everything into loosely strung sentences - will revise tomorrow}

\amtaonly{misc. points that don't have full paragraphs...}

We have not yet implemented the vast majority linguistic constructions in our rudimentary/primitive generator (for example, past tense is only partially supported).
Future development may be expedited by adapting work from existing rule-based MT or natural language generation systems. 
Dynamic corpus generation may also have a role to play in online or curriculum learning settings.

Deep learning methods like RNN encoder-decoders \mycitep{sutskever:2014,bengio:2014} are but the latest in a succession of corpus-based approaches to MT, following in the footsteps of example-based and statistical machine translation; in this era of Big Data it is unlikely that they will be the last.
But all such methods share a reliance on parallel corpora, and we believe parallel corpus generation will remain relevant pursuit as even newer methods emerge.

As alluded to in Section \ref{subsec:benefits}, parallel corpora have the innate potential for open collaboration; not only can they be expert-sourced to linguists, but they can also crowdsourced to ``amateur linguists.''
One particularly promising application domain is conversational language, for which syntactic and lexical diversity is expected to be relatively low compared with more formal domains. 
Multilingual lexical databases for slang terms, in particular, would be particularly suitable for crowdsourcing.
At the very least, crowdsourcing should be strongly considered for post-editing small corpora used as potentially noise-sensitive components of shared tasks, like development or test sets.
In a shared evaluation task with many language pairs, the most practical way for this is likely to be crowdsourcing.

We believe the most promising path to large-scale parallel corpus creation is through educational crowdsourcing.
As with the success story of reCAPTCHA \mycitep{recaptcha}, we believe this will scale best when there is a natural harmony between data supply and demand.
Unlike other staple NLP tasks like parsing or part-of-speech tagging, MT is particularly well-suited to this, with English being taught as a foreign language in classrooms around the world.
The data format (English itself) is widely agreed upon, and there is even an existing market, a vast untapped resource of student problem sets that is yet to be harnessed by machine learning.
\amtaonly{As a back of the envelope estimate, at a translation rate of about 1 noun per minute (based on our own data input experience), even for a modest user base of 10,000 users, a ten-minute problem set could in principle yield 100,000 data samples.
It seems like the bottleneck would be judicious problem set design.}

Large quantities of supervised data underlie state-of-the-art performance in many NLP tasks.
While current paid approaches like Amazon's Mechanical Turk are already a viable way of obtaining crowdsourced data, we believe it would be well worth the initial investment of time and resources to build a sufficiently compelling and widely used educational app capable of harvesting supervised natural language data from its natural source. 
The cost per data point could be significantly reduced or perhaps even become negative (a profitable app), although the value of the harvested data itself might be worth subsidizing.
We consider the construction of such a potentially game-changing source of supervised NLP data to be an open challenge to the NLP community in general.






% TODO: should I note somewhere how much mature the ML component is, 

% TODO: Wit^3 is not fully capitalized in stupid bibliography
% TODO: wtf, ``chinese'' is not capitalized in stupid bibliography


%\amtaonly{
%\section{AMTA}
%This text only gets added for AMTA. I think it'd be nice to have this for ``filler'' text
%
%If I really am submitting to this conference too, then I can afford (and probably want to?) draw out the length, 
%
%Should I mention that you can kind of discern differences between IWSLT, movie subtitles, and MultiUN from their POS distributions? I would probably have to mention something concrete then...? 
%} % end \amtaonly



\hcomponly{ % hmm, maybe I should use this stuff to pad AMTA too? well, let's not pad just for padding's sake... spare the reviewers
\section{HCOMP}

wait, the HCOMP ``short paper'' needs to be 2 pages at MOST, including references

An open challenge to the community - making a crowdsourcing platform
Should I do a quick back-of-the-envelope calculation on how many child-hours are spent on language-related problem sets?

We believe that for crowdsourcing to be most fruitful, there should a matching incentive to the crowd... 
At the risk of sounding like a sales pitch, what if the crowd wanted to pay you to give you data, like Tom Sawyer?
The 2015 paper only tagged like 200 words... well, they were trying something WordNet style... but it just goes to show how slow it is...
WordNet has shit tons of senses, but only a handful of them are common... we want the most common senses here, not completeness.
For rare senses, we can make custom templates.
The semantics we desire are more lightweight (?) - just enough to generate surface forms; don't need to do complex logical reasoning on it.

Rare nouns are the biggest source of lexical diversity.
VERY expensive to do with a small team.
Very ripe for crowdsourcing, but not necessarily by MOOCs, as proposed by LearnWork - problem sets seem more appropriate here.
With a sufficiently compelling problem set app, schools could possibly be willing to pay subscription fees to use this app... (hmm, I see a problem - schools don't have that much money... well, that's where scale comes into play?) well, at the very least, if you make it free, you still get their data
The question seems not so much ``should we'' as much as ``why aren't we doing this yet?'' - at least in the context of NLP.
Unlike some traditional machine learning tasks, which use either sensory or observed data as input, human writers ARE the primary and most natural source of linguistic data.
You wouldn't want to crowdsource bitmaps of cats.
And even if you were crowdsourcing photos of cats, or even photo labeling, that's not really something students do beyond preschool.
But to master one's native tongue takes like 10 years, and there's a BUNCH of unharnessed labor there.

} % end \hcomponly





% these are so short that it's probably better to have it in literally instead of using \input
\if \buildtarget \buildamta    
    \small
    \bibliographystyle{apalike}
\else \if \buildtarget \buildemnlp
    %Note that emnlp2016.bst chokes if there are 0 citations in your paper.    
    %\mynewcite{moses}    
    \bibliographystyle{emnlp2016}
    
\else \if \buildtarget \buildhcomp    
    \bibliographystyle{aaai}
\else
    % some extra error checking, which could not be used with emnlp2016.sty
    \GenericError{}{Invalid buildtarget - \buildtarget}
\fi \fi \fi

\bibliography{references}

\end{document}
