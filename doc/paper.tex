% Yes, this requires flipping flags to compile for different conferences...
% But you can press Ctrl+T while editing in TeXworks without having to change windows!

% conditional compilation
\def\buildemnlp{0}
\def\buildamta{1}
\def\buildhcomp{2}
\def\buildtarget{\buildemnlp} % required to be a single character for \if

% emnlp2016.sty, included by header-emnlp.tex, does NOT tolerate ANY surrounding \else statements...
% an if/else if/... or ifcase statement would have been preferred...
% but \documentclass is declared in the header file, which results in some hidden error-checking:
    % unsupported \buildtarget will fail to build (``\usepackage before \documentclass'')
    % duplicate target values (e.g., emnlp=amta=0) will fail to build (``two \documentclass commands'')
\if \buildtarget \buildemnlp    
    \input{header-emnlp.tex}
    \newcommand\mycitep[1]{\cite{#1}}     % conference-specific ``glue code''
    \newcommand\mynewcite[1]{\newcite{#1}}
    \newcommand\amtaonly[1]{}
    \newcommand\hcomponly[1]{}
\fi
\if \buildtarget \buildamta    
    \input{header-amta.tex}
    \newcommand\mycitep[1]{\citep{#1}} 
    \newcommand\mynewcite[1]{\cite{#1}}  
    \newcommand\amtaonly[1]{#1}
    \newcommand\hcomponly[1]{}
\fi
\if \buildtarget \buildhcomp    
    \input{header-aaai.tex}
    \newcommand\mycitep[1]{\cite{#1}} 
    \newcommand\mynewcite[1]{\citeauthor{#1}~\shortcite{#1}}  
    \newcommand\amtaonly[1]{}
    \newcommand\hcomponly[1]{#1}
\fi
    
% Add support for snippets of \zh{中文}
\usepackage{CJK}
\usepackage[utf8]{inputenc}
\newcommand\zh[1]{\begin{CJK}{UTF8}{gbsn}#1\end{CJK}}

% this would allow clickable urls for my convenience - but many conferences forbid them?
% \usepackage{hyperref}


\begin{document}
\maketitle




\begin{abstract}
An increased focus on a

is proposed.

A particular system for doing this is presented.
It allows syntactic complexity via rules.
It also allows high-quality examples via templates, with minor syntactic modifications via rules.

Preliminary results do not show much improvement.
This is because we need to scale up?

Future work is proposed.
In particular, we believe this is amenable to crowdsourcing by ``amateur linguists''.

\end{abstract}



\section{Motivation} 

As early as Jelinek's (in)famous quote during the 1980's that ``{\em Every time I fire a linguist, my performance goes up},'' both linguists and linguistics have been increasingly marginalized in natural language processing (NLP). 
More recently, for example, \mynewcite{collobert:2011} mention the possibility of ``{\em [ideally]...to learn from letter sequences rather than words},'' illustrating an AI-motivated desire to move away from even the most fundamental of linguistic concepts.
A specific instance of this overall trend is the gradual shift of machine translation (MT) away from classical linguistic rule-based approaches \mycitep{hutchins:2005} to contemporary machine learning approaches, which underlie statistical phrase-based systems like Moses \mycitep{moses} and more recent neural network systems \mycitep{sutskever:2014,bengio:2014}.  
But in the end, even the most cutting-edge machine learning MT system relies crucially on multilingual parallel corpora, typically taken from external sources like multilingual parliamentary proceedings. 
This results in what we perceive to be a disproportionate amount of effort being dedicated to the machine learning component of machine translation, with input corpora being treated almost as an afterthought.\footnote{
    Of course, these parallel corpora represent hundreds of translator-hours of work. 
    However, these translations are generally optimized for human consumption, not machine learning, with no regard for such niceties as sentence alignment.
    This is illustrated briefly in Section \ref{subsec:case_study}.
    }

In this paper, we propose bringing linguists back from a premature exile to craft high-quality, ``designer'' synthetic parallel corpora for machine translation.
Such a division of labor would let linguists be linguists, with MT system development proceeding in parallel.  
In a sense, the use of external corpora has ``outsourced'' the rule-based MT linguist's job to third-party translators, who implicitly encode linguistic information in parallel corpora.  
What we propose is to bring these jobs back ``in-house'', in the updated context of corpus-based MT.

% TODO: i haven't talked yet about the unreasonable effectiveness of in-domain data...
% oh is that the ``upsides'' section?


% although it is ``concrete'', this section will probably be the first to get cut if I run OVER length (which had previously been unimaginable, but is starting to look more and more likely)
% this is some stupid little side note I just noticed a couple weeks ago, but it is expanding to a huge size, because it's like the only concrete ``positive'' result I have...
\subsection{Case Study}
\label{subsec:case_study}

% TODO: don't need to go into this much detail, do you? This would be the first section to trim, if it runs OVER length
% To find an example of suboptimally translated parallel corpora in the literature, one need not look far.

As an example of suboptimal parallel corpora being used for machine translation, consider the official Chinese to English baseline translation of the {\small \tt tst2010} data set\footnote{https://wit3.fbk.eu/score.php?release=2014-01} from the TED talks corpus, described in greater detail in Section \ref{sec:experiments}.
As a very crude way of assessing translation quality, we used the official evaluation software MultEval \mycitep{multeval} to compute scores for each translated line individually.
Upon manual inspection, the worst scoring individual lines (with TER scores over 150\%) were almost all found to correspond to poor human translations on the Chinese side of the gold corpus.\footnote{
    High TER scores indicated misaligned sentences as well as clerical errors.  In particular, the lecture with {\small \tt talkid} 767 had dozens of erroneous lines, in some of the worst cases having the same Chinese sentence copy/pasted three times on the same line. 
    The baseline Moses system tries valiantly, but it ultimately scores poorly for trying to produce a good machine translation of a poor human translation.
    Alternatively, one can interpret these errors as an additional source of noise in the human encoding of English into Chinese.
}
Such translations might suffice for viewing translated videos monolingually, but they are far from ideal for training a machine translation system.

One might expect such errors to be relatively well-tolerated in large training corpora. 
Nevertheless, this is particularly unsettling because the {\small \tt tst2010} data set has been used as the official development/tuning set for the past several IWSLT evaluation campaigns. 
It is also unclear what effect such noisy test corpora have on MT system evaluation and comparison.

In any case, the fact that these errors have persisted this long ({\small \tt tst2010} has been distributed with only minor changes for every IWSLT Chinese-English evaluation campaign since 2012) is a testament to how insidious these errors can be, even for a leading international conference. 
At the very least, this suggests a need for more careful post-editing of bitexts, particularly for development and test corpora.
More ideal, though, would be the creation of parallel corpora for the express purpose of training, tuning, and evaluating data-driven MT systems.



% TODO: display an example bad-sentence pair? If I don't, then what was the point of getting zh fonts working?? sigh

% I guess don't be concrete - don't say it... because then, you have to be more concrete about it...
	% I can elaborate if reviewers ask... but for now, let's just be concise...

% TODO: table with change; also list best and worst 
% caption should mention that the difference between 
    % ALTHOUGH you would expect that it would improve all models, it's not clear whether...err? hard to be precise here.
%Removing TED talk number 767 (over 200 lines), as well as 15 hand-picked lines with bad TER scores, 
%First, note that the scores improve, which is consistent with the observation that 
% something to keep in mind in case reviewers ask
%This results in a significant TER score change that is almost as large as the score difference between the highest and lowest ranking system from the evaluation campaign.  
%BLEU scores \mycitep{bleu}, on the other hand, did not change much.
% no, omit this, because DUH, if you change the test set, the scores will change. (it's like changing your ruler.)
% TODO: table and reference
%\begin{table}[]
%\centering
%\label{table:tst2010}
%\begin{tabular}{|l|l|l|}
%\hline
% & BLEU & TER  \\ \hline
%Full {\small \tt tst2010} & 11.2 & 77.0  \\ \hline
%Purged {\small \tt tst2010} & 11.8 & 71.3  \\ \hline
%%\hline
%%{\small \tt tst2014} &  &   \\ \hline
%% &  &   \\ \hline
%\end{tabular}
%\caption{By comparison, the difference between the baseline TER score and (not shown) } % TODO: caption
%\end{table}
% TODO: table of scores after purging. This is pretty crucial? why?
	% i think i THOUGHT this was crucial because i had to back up my claims concretely...
	% but the claim from the data is tenuous at best - of course the scores change if the test set changes! so WHAT?
	% BESIDES, I tried training and tuning with purged corpora, and the difference was minimal. it's quite possible that people just don't CARE





\subsection{Potential Benefits}
\label{subsec:benefits}

Large, high quality synthetic parallel corpora could be useful as:

\begin{itemize}
\item Training sets for corpus-based MT systems, particularly for low-resource language pairs
\item Benchmark tasks for MT evaluation, in the spirit of the Facebook bAbI tasks in question answering \mycitep{bAbI}
\item Translation memories for computer-aided translation
\item ``Language archives'': New NLP methods are developed every few years; linguistic theories change every decade or two; parallel corpora live forever (e.g., the Rosetta Stone).
\end{itemize}
% sigh, this set of bullets seems much clearer than all those stupid sentences that came before

We believe that natural language processing in general and machine translation in particular are particularly amenable to artifical data generation. 
Unlike many machine learning applications (image recognition, finance), NLP data sets are manmade to begin with; humans themselves {\em are} the primary natural producers and consumers of raw written language data.
Translation is further privileged as perhaps the only NLP task for which millions of people (foreign-language learners) already undergo years of formal training, which has implications for scalability via crowdsourcing (Section \ref{sec:future}).

Other practical considerations shine a favorable light on parallel corpus generation.
As the inverse problem of translation, multilingual generation is also an easier task in some senses. 
For example, lexical ambiguity can be largely alleviated,\footnote{
    Ideally the generator would have {\em a priori} knowledge about whether it is writing a sentence pair about, say,  a (financial) bank or a (river) bank.
} and segmentation of languages like Chinese \mycitep{segmenter:2005,segmenter:2008} becomes a non-issue.
Parallel corpora also provide a natural avenue through which to interface contemporary MT systems with the countless linguist-years of expertise that have been invested into rule-based MT systems, as discussed in Section \ref{sec:related}.
And unlike the various competing MT methods, which are by nature competitive, corpora are inherently collaborative.

This paper is organized as follows.
Section \ref{sec:related} discusses previous related work,
Section \ref{sec:generator} describes our first attempt at implementing a parallel corpus generator,
Section \ref{sec:experiments} gives preliminary results, 
and Section \ref{sec:future} proposes future work.



\section{Related Work}
\label{sec:related}

The idea of using synthetic parallel corpora in machine translation is hardly new.
Previous attempts at creating synthetic parallel corpora have used rule-based MT (RBMT) systems \mycitep{hu:2007,dugast:2008,murakami:2009,rubino:2014} but the input to the RBMT systems was again dictated by external corpora in the desire to produce immediate results.
A longer-term approach that may pay dividends (Section \ref{subsec:benefits}) would be to run RBMT systems on specially chosen sentences for which they are known to produce high-quality translations, or perhaps eventually directly encoding their rules into carefully crafted parallel corpora.

Our parallel corpus generation system (Section \ref{sec:generator}) is very similar to the recursive equivalence class approach to example-based MT \mycitep{ebmtbook} used by \mynewcite{brown:99}. 
Our generator also uses a somewhat more varied grammar that also admits the use of participial noun modifiers.
But more importantly, we believe this line of work was ahead of its time and consider our main contribution to be the reevaluation of its value to the contemporary machine learning approach to MT.  

Equivalence classing is itself an integral part of the default pipeline of Moses \mycitep{moses}, the open-source statistical MT system, with the {\small \tt mkcls} component \mycitep{och:98} binning words into automatically-generated equivalence classes to alleviate data sparsity issues.

Moreover, as they are upstream of GIZA++, these bins are constructed at the lexical level, not the phrase level, whereas the templates described below can make substitutions at the level of larger constituents and phrases.

Furthermore, it is run at training time and cannot help with unknown words at test time (is this true??)
The only real cure for unknown words is moar data


no straightforward way to apply linguistic modifiers to the classes, relying instead on the syntactic variety of the data corpus to capture linguistic effects.

% USUALLY alignment only, CAN be applied to translation... http://www.statmt.org/moses/?n=Advanced.Models#ntoc6
% uh, so do people already do this??
With some effort, you can use these bins during steps other than alignment, although this is not done by default.
...





























\section{A Primitive Multilingual Generator}
\label{sec:generator}

As a simple example, 
{\small \tt The NP jumped over the moon}

system described below is to have linguists write a template and then levering up with the computer to add lexical and syntactic variation.  

The rudimentary tree structure we adopt is something intermediate between constituency trees and dependency trees, and it was chosen for ease of implementation. 
Modifiers are constructed as constituent-like, but also maintain dependency links to their target words.
[ugh should i bother with this?]
especially if we are just concerned with producing high-quality surface forms, instead of performing complex logical reasoning on the underlying data representation




used for corpus analysis stanford POS tagger (incomplete bibtex entry?) \mycitep{postagger:2003}

Unlike AI-oriented or natural language understanding approaches, our primary goal is to generate parallel corpora of surface forms, offering us a significant amount of flexibility on the method of generation. 
adopt elements of both rule-based and example-based MT systems

Our first attempt, described in Section \ref{sec:generator}, 

rules: syntactic variety

in the spirit of example-based MT system \mycitep{ebmtbook}

We adopt the pragmatic, ``defeatist'' attitude of modeling only straightforward linguistic phenomena compositionally; if the theory and data structures required to model a phenomenon are sufficiently complicated, everything is probably better off just going directly to a translation via template, instead of trying to concoct a sophisticated theory that has to be consistent with all previously theorized phenomena.
The tradeoff is that 
Compositional trees are more automatic-like, but generate more sentence pairs/tuples.
Complex templates are more hand-crafted, but generate fewer pairs/tuples, and are (presumably) harder to combine compositionally (but are they really!?).

%\subsection{Simplistic Linguistics}
We adopt the exceedingly simplistic view of language as nouns, verbs, and descriptions (modifications) thereof.

We focus on declarative indicative present active sentences in the hopes that they can be transformed into other forms in the long run.
Similarly, although we enforce some semantic constraints, our system generally produces nonsensical sentences in the spirit of ``Colorless green ideas sleep furiously''.
based very loosely on the early work of chomsky: transformations, colorless green ideas? 


two sources of complexity
- lexical (semantic?)
- syntactic 

The fact that verbs can serve as nouns (gerundives) and modifiers (participles and modals) is a significant source of added complexity. (footnote?)

Testing section reference: \ref{sec:related} - this requires changing secnumdepth=2 in AAAI

%\subsection{Implementation Details}
Python
YAML for data: very well suited. full Unicode support, and you don't even need to type quotation marks to enclose strings.
with an eye toward scalability
Crude though it may be, we plan on eventually open-sourcing both the generator and its accompanying data





\section{Preliminary Experiments}
\label{sec:experiments}

\subsection{Methods}
As mentioned in 
The IWSLT 2015 version of the WIT$^3$ corpus, which consists of multilingual subtitles of TED talks, covering a wide variety of subjects. 
considered briefly in Section \ref{subsec:case_study}

As prepared lectures, they fall somewhere between the conversational language of movie subtitles and the formal written language of UN resolutions.
The latter are freely available online from the OPUS project \mycitep{opus:2012} as the OpenSubtitles\footnote{http://www.opensubtitles.org/} 2016 \mycitep{opensubtitles:2016} and the MultiUN \mycitep{multiun} corpora, respectively.

We consider the Chinese to English machine translation task, using the 

first 

Out of domain corpus

the latest version of the Stanford Segmenter \mycitep{segmenter:2005,segmenter:2008}

citations needed
IWSLT \mycitep{wit3} - point out that this corpus is freely available
OPUS 
zh\_zh corpus from the OpenSubtitles 2016 corpus , with lines cleaned using some ad hoc heuristics
MultiUN 
Moses \mycitep{moses}

Since we are interested in the relative effect an additional synthetic parallel corpus
we choose a simple baseline system and fix other variables

We configured Moses as described in \mycitep{wit3}, with the exception of changing the target monolingual language model toolkit to KenLM \mycitep{kenlm}, which we found to be faster and stabler than IRSTLM \mycitep{irstlm} at decoding time.
For the remaining unspecified model settings, we used the {\small \tt grow-diag} alignment heuristic recommended by \mynewcite{segmenter:2008} and the {\small \tt mslr-fe} configuration for lexicalized reordering.

The corpus, which is freely available, consists 

IWSLT: TED talks are a series of lectures, which makes them 



heuristically purged zh\_zh, which is a very noisy corpus

\subsection{Results}

2016-02-20 learning curves adding only 130,000 in-domain lines improves scores by like 2.0 BLEU points

2016-03-03-225556 1M zh\_zh lines made scores slightly WORSE?

2016-04-29-093202 1M MultiUN lines only improved scores SLIGHTLY (by like 0.4 BLEU points)

Quantity and maybe even quality (you'd expect MultiUN translations to be better?) matters much less than having data in-domain

Of course, I need to redo (ALL?) these jobs with final segmenter choice

Evaluated using the IWSLT 2015 progress data set (tst2014) to allow comparison with both 2015 and 2014 systems.

training curves (just to prove my controls aren't wrong)
my synthetic corpora don't help. 
but in-domain corpus helps significantly more than out-of-domain. (this is a commonly reported result)

Bottom line
My corpora don't really seem to help, but neither do out-of-domain corpora.

this will easily take up a whole column, because it includes 2 tables...

\section{Proposed Future Work}
\label{sec:future}

I.e., shit I REALLY doubt I have time for...


perhaps using example-based MT techniques \mycitep{ebmtbook} and crowdsourcing to generate them at scale.

For this paper, the lexical database was drawn from the IWSLT training set to maximize bottom-line performance.
[i.e., i don't have TIME to do a general-purpose database]
This is indeed one potential use of this system, for training set amplification.
The envisioned ultimate target domain is (cleaned) everyday speech (maybe movie or TV subtitles?) , for which syntactic and lexical diversity is expected to be relatively low, and unknown words would be particularly suitable for crowdsourcing.

parallel corpus construction has been considered prohibitively expensive, hence the use of pre-translated sources like parliamentary proceedings
we believe it is worth an initial investment to create a scalable system

expert-sourcing by linguists, using the computer to lever up - computer assisted translation, in a sense; maybe computer-amplified
crowdsourcing by ``amateur linguists''


moreover, you're at the mercy of your third-party translators - if you want more data, you gotta wait.
But with a sufficiently compelling educational app/platform, you wouldn't have to wait OR pay for more data

Crowdsourcing also has its place in post-editing, at the very least, for test sets.
In a shared evaluation task with many language pairs, the most practical way for this is probably crowdsourcing.

But must this be so? Vast untapped resource of students or whatever.
an open challenge to the NLP community at large
with a sufficiently compelling and thus widely used educational app 
a potentially game-changing source of supervised data for NLP in general

Well, typical approaches are paid, like Mechanical Turk... but there is a vast untapped source of data in the form of student problem sets
as with the success story of reCAPTCHA \mycitep{recaptcha}, we believe this will scale best when there is a natural harmony between data supply and demand


estimate about 1 noun per minute (my pathetic work speed)

locally weighted linear regression? online learning?
but corpus generation at test time is probably just as noisy as any discriminative method

Instead of the next paragraph, should probably just point out how rudimentary the current ``grammar'' is. 
Future development may be facilitated by adapting existing work from RBMT or NLG. 
Okay, I guess just point out this grammar doesn't even handle something as basic as:

Shit tons of linguistic phenomena that aren't even that uncommon, including but not limited to:
More thorough treatment of articles.
(More) treatment of prepositional phrases.
Conjunctions.
Interrogatives and imperatives.


In the short term
Increasing lexical variation by expanding the lexicon, especially for uncommon words in the training corpus, to alleviate data sparsity.
Increasing syntactic variation in a high-quality way by harvesting more templates, in the old EBMT tradition
Work along these lines is currently underway.


%RNN encoder-decoders are not the first corpus-based MT methodology (EBMT then SMT then this), and it is unlikely in this era of Big Data that they will be the last.




% TODO: Wit^3 is not fully capitalized in stupid bibliography
% TODO: wtf, ``chinese'' is not capitalized in stupid bibliography


\amtaonly{
\section{AMTA}
This text only gets added for AMTA. I think it'd be nice to have this for ``filler'' text

If I really am submitting to this conference too, then I can afford (and probably want to?) draw out the length, 

Should I mention that you can kind of discern differences between IWSLT, movie subtitles, and MultiUN from their POS distributions? I would probably have to mention something concrete then...? 
} % end \amtaonly



\hcomponly{ % hmm, maybe I should use this stuff to pad AMTA too? well, let's not pad just for padding's sake... spare the reviewers
\section{HCOMP}

wait, the HCOMP ``short paper'' needs to be 2 pages at MOST, including references

An open challenge to the community - making a crowdsourcing platform
Should I do a quick back-of-the-envelope calculation on how many child-hours are spent on language-related problem sets?

We believe that for crowdsourcing to be most fruitful, there should a matching incentive to the crowd... 
At the risk of sounding like a sales pitch, what if the crowd wanted to pay you to give you data, like Tom Sawyer?
The 2015 paper only tagged like 200 words... well, they were trying something WordNet style... but it just goes to show how slow it is...
WordNet has shit tons of senses, but only a handful of them are common... we want the most common senses here, not completeness.
For rare senses, we can make custom templates.
The semantics we desire are more lightweight (?) - just enough to generate surface forms; don't need to do complex logical reasoning on it.

Rare nouns are the biggest source of lexical diversity.
VERY expensive to do with a small team.
Very ripe for crowdsourcing, but not necessarily by MOOCs, as proposed by LearnWork - problem sets seem more appropriate here.
With a sufficiently compelling problem set app, schools could possibly be willing to pay subscription fees to use this app... (hmm, I see a problem - schools don't have that much money... well, that's where scale comes into play?) well, at the very least, if you make it free, you still get their data
The question seems not so much ``should we'' as much as ``why aren't we doing this yet?'' - at least in the context of NLP.
Unlike some traditional machine learning tasks, which use either sensory or observed data as input, human writers ARE the primary and most natural source of linguistic data.
You wouldn't want to crowdsource bitmaps of cats.
And even if you were crowdsourcing photos of cats, or even photo labeling, that's not really something students do beyond preschool.
But to master one's native tongue takes like 10 years, and there's a BUNCH of unharnessed labor there.

} % end \hcomponly





% these are so short that it's probably better to have it in literally instead of using \input
\if \buildtarget \buildamta    
    \small
    \bibliographystyle{apalike}
\else \if \buildtarget \buildemnlp
    %Note that emnlp2016.bst chokes if there are 0 citations in your paper.    
    %\mynewcite{moses}    
    \bibliographystyle{emnlp2016}
    
\else \if \buildtarget \buildhcomp    
    \bibliographystyle{aaai}
\else
    % some extra error checking, which could not be used with emnlp2016.sty
    \GenericError{}{Invalid buildtarget - \buildtarget}
\fi \fi \fi

\bibliography{references}

\end{document}
