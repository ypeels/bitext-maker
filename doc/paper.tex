% Yes, this requires flipping flags to compile for different conferences...
% But you can press Ctrl+T while editing in TeXworks without having to change windows!

% conditional compilation
\def\buildemnlp{0}
\def\buildamta{1}
\def\buildhcomp{2}
\def\buildtarget{\buildemnlp} % required to be a single character for \if

% emnlp2016.sty, included by header-emnlp.tex, does NOT tolerate ANY surrounding \else statements...
% an if/else if/... or ifcase statement would have been preferred...
% but \documentclass is declared in the header file, which results in some hidden error-checking:
    % unsupported \buildtarget will fail to build (``\usepackage before \documentclass'')
    % duplicate target values (e.g., emnlp=amta=0) will fail to build (``two \documentclass commands'')
\if \buildtarget \buildemnlp    
    \input{header-emnlp.tex}
    \newcommand\mycitep[1]{\cite{#1}}     % conference-specific ``glue code''
    \newcommand\mynewcite[1]{\newcite{#1}}
    \newcommand\amtaonly[1]{}
    \newcommand\hcomponly[1]{}
\fi
\if \buildtarget \buildamta    
    \input{header-amta.tex}
    \newcommand\mycitep[1]{\citep{#1}} 
    \newcommand\mynewcite[1]{\cite{#1}}  
    \newcommand\amtaonly[1]{#1}
    \newcommand\hcomponly[1]{}
\fi
\if \buildtarget \buildhcomp    
    \input{header-aaai.tex}
    \newcommand\mycitep[1]{\cite{#1}} 
    \newcommand\mynewcite[1]{\citeauthor{#1}~\shortcite{#1}}  
    \newcommand\amtaonly[1]{}
    \newcommand\hcomponly[1]{#1}
\fi
    
% Add support for snippets of \zh{中文}
\usepackage{CJK}
\usepackage[utf8]{inputenc}
\newcommand\zh[1]{\begin{CJK}{UTF8}{gbsn}#1\end{CJK}}

% this would allow clickable urls for my convenience - but many conferences forbid them?
% \usepackage{hyperref}


\begin{document}
\maketitle




\begin{abstract}
An increased focus on 

is proposed.

Preliminary results do not show much improvement.
This is because we need to scale up?
Future work is proposed.

\end{abstract}







\section{Motivation} % Previously called ``Introduction''


As early as Jelinek's (in)famous 1980's quote, ``{\em Every time I fire a linguist, my performance goes up},'' both linguists and linguistics have been increasingly marginalized in natural language processing. 
More recently, for example, \mynewcite{collobert 2011 footnote 8} mention the possibility of ``{\em [ideally]...to learn from letter sequences rather than words},'' illustrating an AI-motivated desire to move away from even fundamental linguistic concepts.
A specific case of this trend is the gradual shift away from classical linguistic rule-based machine translation (MT) \mycitep{hutchins} to  contemporary machine learning approaches, such as staistical phrase-based systems like Moses \mycitep{moses} and more recent deep learning approaches \mycitep{sutskever, bengio}.

In the end, though, machine learning is learning from data, and even the most bleeding-edge MT system relies crucially on multilingual parallel corpora.  In this paper, we propose recalling linguists from their increasing exile from MT to handle what they know best: language. 
Specifically, we believe that machine translation could benefit from high-quality, ``designer'' parallel corpora that have been hand-crafted by linguists. This could take the form of amplified training sets in the short run, and perhaps a ``universal translation memory'' in the long run.




This division of labor would let linguists be linguists, with MT system development proceeding in parallel, consuming the linguists' work downstream.

UGH I have nothing really concrete to show here... this is like an opinion paper at this point...
Although I guess I do have this stupid system I worked on, and I'll push it to the web, and they can laugh at how pathetic it is.

Significantly more developer-months go into the MT system.\footnote{Of course, hundreds of translator-hours }

niceties like sentence alignment  
(Well, granted, there are human translators who are spending many man-hours generating sentence pairs. But they're not intended for MT.)
There have also been attempts to 

My main point seems to be that nobody makes high quality and quantity parallel corpora JUST for machine translation, and it seems like that's sorely needed for both training and benchmarking.
Even the fanciest new machine learning model is dead in the water if it doesn't have enough data.

disproportionate amount of attention being paid to the machine learning
not enough attention being paid to the corpora themselves

I think this first part of the introduction should be short, and then some subsections below?

Traditionally, corpus construction has been considered prohibitively expensive, ``scavenged'' from other sources like parliamentary proceedings or lecture subtitles.
But must this be so? Vast untapped resource of students or whatever.


No, my Return of the Linguist had more to do with how RBMT systems have fallen out of favor in research
banishment has been premature? weren't using the linguist in the right way?


\section{Single-Line Scoring}

The machine translation task we consider in this paper is the IWSLT shared task 

A brief analysis of corpus quality

\mycitep{multeval}

Do I want to display an example bad-sentence pair? 
Some of the lines are duplicated. Examining 
Just mention for now that there is an entire lecture that is bad
Also hand-purged several other sentences

TODO: table of scores after purging

This results in a significant TER score change that is almost as large as the score difference between the highest and lowest ranking system from the evaluation campaign.  
Interestingly, BLEU scores stayed relatively robust.

It is a testament to how indisidious these errors can be that they persisted this long (tst2010 was distributed as recently as IWSLT2015
This underscores the need for human post-editing, at the very least, for test sets.
In a shared evaluation task with many language pairs, the most practical way for this is probably crowdsourcing.

It also demonstrates the tremendous imbalance between linguistic effort and machine learning effort/investment



\subsection{Upsides, Potential Applications}



Such synthetic parallel corpora can also be viewed as benchmark tasks (need to cite Facebook bAbI).
This is actually probably more important than you'd think, because looking at IWSLT15, some of the human translations are VERY loose.
So there's another element of noise in the data, and it's one that no amount of downstream machine learning can get rid of.
(Although people have tried corpus evaluation...)

% interesting, but probably confusing to readers?
%In the language of agile software development, test-driven MT system development, in which linguists pose sentence pairs, and MT developers work to translate them


Focusing on corpus generation would allow linguists to focus their attention on significant linguistic phenomena

methods every few years; linguistic theories, representations every decade or so; parallel corpora live forever

lexical ambiguity is naturally alleviated

RBMT - countless developer-years of expertise

unlike all the different competing methods, it seems that a ``universal translation memory'' would be inherently collaborative (methods are competitive by nature, whereas data can be collaborative)


%, and with the two meeting at the golden railroad spike of the parallel corpus. Apparently I've liked this stupid thing for 10+ years
% on capturing the most statistically prevalent linguistic phenomena, while insulating them from the strengths and weaknesses of various MT implementations. (wait, is that a good thing? don't you need some feedback as to what KIND of corpus the MT system needs?)
%With the ALPAC report's pessimistic outlook on MT arguably responsible for sparking decades of fundamental linguistics research in the tradition of Chomsky, it is only fitting that we now bring linguistics back?
%RNN encoder-decoders are not the first corpus-based MT methodology (EBMT then SMT then this), and it is unlikely in this era of Big Data that they will be the last.
% For instance, EBMT suffers from relatively poor generalization and SMT has problems with nonlocality.
% (RNN built on relatively shaky ground, since word vectors don't QUITE make sense for function words?)
% Seems somewhat foolhardy to marginalize the domain experts (no, lots of deep learning advances have been made by IGNORING expert-created representations)
% One might conjecture that the fundamental problem in natural language processing is one of data representation
%Return of the linguist: it is relatively easy to come up with examples that industrial-grade systems fail. (hmm,)
%he was shown the car \zh{他 给 展示 了 车}?



 








\section{Related Work}
\label{sec:related}

Nice tip: ONE LINE PER SENTENCE.

Unlike more AI-oriented or natural language understanding approaches, we are solely concerned here with the generation of surface forms

At some point, I will almost definitely be citing Moses \mycitep{moses}

the latest version of the Stanford Segmenter \mycitep{segmenter:2005,segmenter:2008}

This aspect of our work is perhaps closest in spirit to the equivalence classes used by \mynewcite{brown:99} to reduce the data requirements of an EBMT system \mycitep{ebmtbook}.

In the Moses toolchain, the {\small \tt mkcls} component \mycitep{och:98} performs this process in reverse, binning words into automatically-generated equivalence classes to alleviate data sparsity issues.
% USUALLY alignment only, CAN be applied to translation... http://www.statmt.org/moses/?n=Advanced.Models#ntoc6
% uh, so do people already do this??
With some effort, you can use these bins during steps other than alignment, although this is not done by default.
...
Moreover, as they are upstream of GIZA++, these bins are constructed at the lexical level, not the phrase level, whereas the templates described below can make substitutions at the level of larger constituents and phrases.
Furthermore, it is run at training time and cannot help with unknown words at test time (is this true??)
The only real cure for unknown words is moar data


some neural network MT paper?

aha, yoshua bengio's "curse of dimensionality" - becomes a blessing?  
Unlike many machine learning tasks (image recognition, finance), NLP data are purely artificial/manmade.  New paradigm?  Not enough attention has been paid IMO to semi-automated corpus generation.  Well, typical approaches are paid, like Mechanical Turk... but there is a vast untapped source of data in the form of student problem sets, in the spirit of reCAPTCHA (citation!?)
NLP is unique among machine learning tasks in that humans are not only consumers, but also producers of data - the alpha and the omega, the beginning and the end.
MT is further unique among NLP in that both input and output representations are widely accepted (modulo dialectical variations) - the source and target languages themselves.
People on the street would agree to the representations.


Translation is perhaps the only NLP task for which millions of people already undergo years of training

other approaches to synthetic corpora, which are essentially bootstrapped from other MT systems

Hardly a new idea ... but the emphasis here is on the resulting corpus





maybe the latest Hutchins overview or nutshell?


chomsky: transformations, colorless green ideas


size doesn't always matter - those massive OpenSubtitles corpora don't help
- uh, is the problem quality? should i run some Moses runs with a small portion of MultiUN to see how quality improves?

lots of things are resolved naturally when considering generation instead of discrimination
- ambiguity (you know what you want to say)
- segmentation (you know what ``phrases'' you wanted to output)

WELL... issues of ambiguity are alleviated, but only somewhat (e.g., for a sentence about a bank, the generator should know whether it's a building). 
BUT ambiguous attachments can still be generated (e.g., the [animal] with the [animal] with the overbite), and it would still take additional logic for the system to recognize this, much less generate all possible translations...
Maybe just emphasize the fact that lexical ambiguity is virtually eliminated, although structural or semantic ambiguity is still possible in some cases...
But it still seems like it'll be fewer cases... The [person] saw the dog with(by means of) the [vision implement]


Can think of this as something intermediate between fully automatic (RBMT, or semi-supervised SMT) and fully hand-crafted (human translation, which is the norm).
Compositional trees are more automatic-like, but generate more sentence pairs/tuples.
Complex templates are more hand-crafted, but generate fewer pairs/tuples, and are (presumably) harder to combine compositionally (but are they really!?).

The tree structure we adopt is something intermediate between constituency trees and dependency trees, and it was chosen for ease of implementation.

We adopt the pragmatic, ``defeatist'' attitude of modeling only straightforward linguistic phenomena compositionally; if the theory and data structures required to model a phenomenon are sufficiently complicated, everything is probably better off just going directly to a translation via template, instead of trying to concoct a sophisticated theory that has to be consistent with all previously theorized phenomena.

This is (probably? ref??) unlike other NLG work in that our emphasis is on surface forms, not the underlying data representation.


\section{Model}

used for corpus analysis stanford POS tagger (incomplete bibtex entry?) \mycitep{postagger:2003}

\subsection{Simplistic Linguistics}
We adopt the exceedingly simplistic view of language as nouns, verbs, and descriptions (modifications) thereof.

two sources of complexity
- lexical (semantic?)
- syntactic 

The fact that verbs can serve as nouns (gerundives) and modifiers (participles and modals) is a significant source of added complexity. (footnote?)


I guess I should use placeholder graphics for now, since deadlines for camera-ready are quite a bit later.
EMNLP: 6/3 submission, 7/29 notification, 9/23 camera-ready
AMTA: 6/27 submission, 8/8 notification, 9/12 camera-ready

Testing section reference: \ref{sec:related} - this requires changing secnumdepth=2 in AAAI

\section{Experimental Methods}

Out of domain corpus

citations needed
IWSLT - point out that this corpus is freely available
OPUS zh\_zh
MultiUN
Moses

parameters used

The corpus, which is freely available, consists of multilingual subtitles of TED talks, which are prepared lectures on a wide variety of subjects.

IWSLT: TED talks are a series of lectures, which makes them something intermediate between the conversational language of movie subtitles and the formal written language of UN resolutions.



heuristically purged zh\_zh, which is a very noisy corpus

\section{Results}

2016-02-20 learning curves adding only 130,000 in-domain lines improves scores by like 2.0 BLEU points

2016-03-03-225556 1M zh\_zh lines made scores slightly WORSE?

2016-04-29-093202 1M MultiUN lines only improved scores SLIGHTLY (by like 0.4 BLEU points)

Quantity and maybe even quality (you'd expect MultiUN translations to be better?) matters much less than having data in-domain

Of course, I need to redo (ALL?) these jobs with final segmenter choice

Evaluated using the IWSLT 2015 progress data set (tst2014) to allow comparison with both 2015 and 2014 systems.


\section{Proposed Future Work}

I.e., shit I REALLY doubt I have time for...



For this paper, the lexical database was drawn from the IWSLT training set to maximize bottom-line performance.
[i.e., i don't have TIME to do a general-purpose database]
This is indeed one potential use of this system, for training set amplification.
The envisioned ultimate target domain is (cleaned) everyday speech (maybe movie or TV subtitles?) , for which syntactic and lexical diversity is expected to be relatively low, and unknown words would be particularly suitable for crowdsourcing.

crowdsourcing for ``amateur linguists''

AHA, what you've really done is traded in-house linguists who coded rules for outsourced linguists who churn out data.
Is that really optimal?
Better off bringing corpus writers back in-house, where possible.

estimate about 1 noun per minute (my pathetic work speed)


Instead of the next paragraph, should probably just point out how rudimentary the current ``grammar'' is. 
Future development may be facilitated by adapting existing work from RBMT or NLG. 
Okay, I guess just point out this grammar doesn't even handle something as basic as:

Shit tons of linguistic phenomena that aren't even that uncommon, including but not limited to:
More thorough treatment of articles.
(More) treatment of prepositional phrases.
Conjunctions.
Interrogatives and imperatives.
Pronouns - implemented as names here...? wait, but what about I/me? ugh

In the short term
Increasing lexical variation by expanding the lexicon, especially for uncommon words in the training corpus, to alleviate data sparsity.
Increasing syntactic variation in a high-quality way by harvesting more templates, in the old EBMT tradition
Work along these lines is currently underway.



\amtaonly{
\section{AMTA}
This text only gets added for AMTA. I think it'd be nice to have this for ``filler'' text

If I really am submitting to this conference too, then I can afford (and probably want to?) draw out the length, 

Should I mention that you can kind of discern differences between IWSLT, movie subtitles, and MultiUN from their POS distributions? I would probably have to mention something concrete then...? 
} % end \amtaonly



\hcomponly{ % hmm, maybe I should use this stuff to pad AMTA too? well, let's not pad just for padding's sake... spare the reviewers
\section{HCOMP}

wait, the HCOMP ``short paper'' needs to be 2 pages at MOST, including references

An open challenge to the community - making a crowdsourcing platform
Should I do a quick back-of-the-envelope calculation on how many child-hours are spent on language-related problem sets?

We believe that for crowdsourcing to be most fruitful, there should a matching incentive to the crowd... 
At the risk of sounding like a sales pitch, what if the crowd wanted to pay you to give you data, like Tom Sawyer?
The 2015 paper only tagged like 200 words... well, they were trying something WordNet style... but it just goes to show how slow it is...
WordNet has shit tons of senses, but only a handful of them are common... we want the most common senses here, not completeness.
For rare senses, we can make custom templates.
The semantics we desire are more lightweight (?) - just enough to generate surface forms; don't need to do complex logical reasoning on it.

Rare nouns are the biggest source of lexical diversity.
VERY expensive to do with a small team.
Very ripe for crowdsourcing, but not necessarily by MOOCs, as proposed by LearnWork - problem sets seem more appropriate here.
With a sufficiently compelling problem set app, schools could possibly be willing to pay subscription fees to use this app... (hmm, I see a problem - schools don't have that much money... well, that's where scale comes into play?) well, at the very least, if you make it free, you still get their data
The question seems not so much ``should we'' as much as ``why aren't we doing this yet?'' - at least in the context of NLP.
Unlike some traditional machine learning tasks, which use either sensory or observed data as input, human writers ARE the primary and most natural source of linguistic data.
You wouldn't want to crowdsource bitmaps of cats.
And even if you were crowdsourcing photos of cats, or even photo labeling, that's not really something students do beyond preschool.
But to master one's native tongue takes like 10 years, and there's a BUNCH of unharnessed labor there.

} % end \hcomponly





% these are so short that it's probably better to have it in literally instead of using \input
\if \buildtarget \buildamta    
    \small
    \bibliographystyle{apalike}
\else \if \buildtarget \buildemnlp
    %Note that emnlp2016.bst chokes if there are 0 citations in your paper.    
    %\mynewcite{moses}    
    \bibliographystyle{emnlp2016}
    
\else \if \buildtarget \buildhcomp    
    \bibliographystyle{aaai}
\else
    % some extra error checking, which could not be used with emnlp2016.sty
    \GenericError{}{Invalid buildtarget - \buildtarget}
\fi \fi \fi

\bibliography{references}

\end{document}
